{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; background-color: #D9EDF7; border: 1px solid #CFCFCF; text-align: left; padding: 10px;\">\n",
    "      <b>Renewable power plants: Download and process notebook</b>\n",
    "      <ul>\n",
    "        <li><a href=\"main.ipynb\">Main notebook</a></li>\n",
    "        <li>Download and process notebook</li>\n",
    "        <li><a href=\"validation_and_output.ipynb\">Validation and output notebook</a></li>\n",
    "      </ul>\n",
    "      <br>This notebook is part of the <a href=\"http://data.open-power-system-data.org/renewable_power_plants\"> Renewable power plants Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This script downlads and extracts the original data of renewable power plant lists from the data sources, processes and merges them. It subsequently adds the geolocation for each power plant. Finally it saves the DataFrames as pickle-files. Make sure you run the download and process Notebook before the validation and output Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Script-setup\" data-toc-modified-id=\"Script-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Script setup</a></span></li><li><span><a href=\"#Settings\" data-toc-modified-id=\"Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Settings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-download-option\" data-toc-modified-id=\"Choose-download-option-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choose download option</a></span></li><li><span><a href=\"#OPSD-server-download-configuration\" data-toc-modified-id=\"OPSD-server-download-configuration-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>OPSD server download configuration</a></span></li><li><span><a href=\"#Download-function\" data-toc-modified-id=\"Download-function-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Download function</a></span></li><li><span><a href=\"#Save-functions\" data-toc-modified-id=\"Save-functions-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Save functions</a></span></li><li><span><a href=\"#Setup-translation-dictionaries\" data-toc-modified-id=\"Setup-translation-dictionaries-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Setup translation dictionaries</a></span></li><li><span><a href=\"#Setup-source-list\" data-toc-modified-id=\"Setup-source-list-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Setup source list</a></span></li><li><span><a href=\"#Visualizer\" data-toc-modified-id=\"Visualizer-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Visualizer</a></span></li></ul></li><li><span><a href=\"#Download-and-process-per-country\" data-toc-modified-id=\"Download-and-process-per-country-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Download and process per country</a></span><ul class=\"toc-item\"><li><span><a href=\"#Germany-DE\" data-toc-modified-id=\"Germany-DE-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Germany DE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-information-and-choose-columns\" data-toc-modified-id=\"Add-information-and-choose-columns-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Add information and choose columns</a></span></li><li><span><a href=\"#Merge-DataFrames\" data-toc-modified-id=\"Merge-DataFrames-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Merge DataFrames</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Transform-electrical-capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical-capacity-from-kW-to-MW-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Transform electrical capacity from kW to MW</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-coordinates-by-postcode\" data-toc-modified-id=\"Get-coordinates-by-postcode-3.1.7.1\"><span class=\"toc-item-num\">3.1.7.1&nbsp;&nbsp;</span>Get coordinates by postcode</a></span></li><li><span><a href=\"#Transform-geoinformation\" data-toc-modified-id=\"Transform-geoinformation-3.1.7.2\"><span class=\"toc-item-num\">3.1.7.2&nbsp;&nbsp;</span>Transform geoinformation</a></span></li></ul></li><li><span><a href=\"#Clean-data\" data-toc-modified-id=\"Clean-data-3.1.8\"><span class=\"toc-item-num\">3.1.8&nbsp;&nbsp;</span>Clean data</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.1.9\"><span class=\"toc-item-num\">3.1.9&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.1.10\"><span class=\"toc-item-num\">3.1.10&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Denmark-DK\" data-toc-modified-id=\"Denmark-DK-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Denmark DK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source-and-missing-information\" data-toc-modified-id=\"Add-data-source-and-missing-information-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Add data source and missing information</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Merge-DataFrames-and-choose-columns\" data-toc-modified-id=\"Merge-DataFrames-and-choose-columns-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Merge DataFrames and choose columns</a></span></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.2.7\"><span class=\"toc-item-num\">3.2.7&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.2.8\"><span class=\"toc-item-num\">3.2.8&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.2.9\"><span class=\"toc-item-num\">3.2.9&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#France-FR\" data-toc-modified-id=\"France-FR-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>France FR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Rearrange-columns-and-translate-column-names\" data-toc-modified-id=\"Rearrange-columns-and-translate-column-names-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Rearrange columns and translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Municipality-(INSEE)-code-to-lon/lat\" data-toc-modified-id=\"Municipality-(INSEE)-code-to-lon/lat-3.3.5.1\"><span class=\"toc-item-num\">3.3.5.1&nbsp;&nbsp;</span>Municipality (INSEE) code to lon/lat</a></span></li></ul></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.3.7\"><span class=\"toc-item-num\">3.3.7&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Poland-PL\" data-toc-modified-id=\"Poland-PL-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Poland PL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Rearrange-data-from-rft-file\" data-toc-modified-id=\"Rearrange-data-from-rft-file-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Rearrange data from rft-file</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-levels\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-levels-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Translate values and harmonize energy source levels</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Switzerland-CH\" data-toc-modified-id=\"Switzerland-CH-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Switzerland CH</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Harmonize-energy-source-hierarchy-and-translate-values\" data-toc-modified-id=\"Harmonize-energy-source-hierarchy-and-translate-values-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Harmonize energy source hierarchy and translate values</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.5.6\"><span class=\"toc-item-num\">3.5.6&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Select-columns-to-keep\" data-toc-modified-id=\"Select-columns-to-keep-3.5.7\"><span class=\"toc-item-num\">3.5.7&nbsp;&nbsp;</span>Select columns to keep</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.5.8\"><span class=\"toc-item-num\">3.5.8&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.5.9\"><span class=\"toc-item-num\">3.5.9&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#United-Kingdom-UK\" data-toc-modified-id=\"United-Kingdom-UK-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>United Kingdom UK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-Read\" data-toc-modified-id=\"Download-and-Read-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Download and Read</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.6.4\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonise-energy-source-levels-1-3-and-technology\" data-toc-modified-id=\"Translate-values-and-harmonise-energy-source-levels-1-3-and-technology-3.6.5\"><span class=\"toc-item-num\">3.6.5&nbsp;&nbsp;</span>Translate values and harmonise energy source levels 1-3 and technology</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.6.6\"><span class=\"toc-item-num\">3.6.6&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cases-with-unknown-Easting-and-Northing-coordinates\" data-toc-modified-id=\"Cases-with-unknown-Easting-and-Northing-coordinates-3.6.6.1\"><span class=\"toc-item-num\">3.6.6.1&nbsp;&nbsp;</span>Cases with unknown Easting and Northing coordinates</a></span></li><li><span><a href=\"#Cases-for-approximation\" data-toc-modified-id=\"Cases-for-approximation-3.6.6.2\"><span class=\"toc-item-num\">3.6.6.2&nbsp;&nbsp;</span>Cases for approximation</a></span></li><li><span><a href=\"#Visualize-the-data\" data-toc-modified-id=\"Visualize-the-data-3.6.6.3\"><span class=\"toc-item-num\">3.6.6.3&nbsp;&nbsp;</span>Visualize the data</a></span></li></ul></li><li><span><a href=\"#Keep-only-the-columns-of-interest\" data-toc-modified-id=\"Keep-only-the-columns-of-interest-3.6.7\"><span class=\"toc-item-num\">3.6.7&nbsp;&nbsp;</span>Keep only the columns of interest</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.6.8\"><span class=\"toc-item-num\">3.6.8&nbsp;&nbsp;</span>Save</a></span></li></ul></li></ul></li><li><span><a href=\"#Zip-the-raw-data\" data-toc-modified-id=\"Zip-the-raw-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Zip the raw data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '2019-04-05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import posixpath\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import re\n",
    "import zipfile\n",
    "import pickle\n",
    "import urllib\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utm  # for transforming geoinformation in the utm format\n",
    "import requests\n",
    "import fake_useragent\n",
    "from string import Template\n",
    "from IPython.display import display\n",
    "import xlrd\n",
    "import bs4\n",
    "import bng_to_latlon\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# for visualizing locations on maps\n",
    "import cartopy.crs as ccrs \n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io import shapereader\n",
    "import geopandas\n",
    "import shapely\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%d %b %Y %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create input, intermediate and output folders if they don't exist.\n",
    "# If the paths are relative, the correspoding folders will be created\n",
    "# inside the current working directory.\n",
    "input_directory_path = os.path.join('input', 'original_data')\n",
    "intermediate_directory_path = 'intermediate'\n",
    "output_directory_path = os.path.join('output', 'renewable_power_plants')\n",
    "\n",
    "os.makedirs(input_directory_path, exist_ok=True)\n",
    "os.makedirs(intermediate_directory_path, exist_ok=True)\n",
    "os.makedirs(output_directory_path, exist_ok=True)\n",
    "\n",
    "# Create the folder to which the Eurostat files with data at the level of the whole EU/Europe\n",
    "#are going to be downloaded\n",
    "eurostat_eu_directory_path = os.path.join('input', 'eurostat_eu')\n",
    "os.makedirs(eurostat_eu_directory_path, exist_ok=True)\n",
    "\n",
    "# Define the path of the file with the list of sources.\n",
    "source_list_filepath = os.path.join('input', 'sources.csv')\n",
    "\n",
    "# Import the utility functions and classes from the util package\n",
    "import util.helper\n",
    "from util.visualizer import visualize_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose download option\n",
    "The original data can either be downloaded from the original data sources as specified below or from the opsd-Server. Default option is to download from the original sources as the aim of the project is to stay as close to original sources as possible. However, if problems with downloads e.g. due to changing urls occur, you can still run the script with the original data from the opsd_server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from = 'original_sources'\n",
    "#download_from = 'opsd_server' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the download links\n",
    "\n",
    "The download link for the UK is updated at the end of each quarter by the source provider, BEIS. We keep up with those changes by extracting the download link automatically from the web page it is on. That way, the link does not have to be updated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_csv(source_list_filepath)\n",
    "uk_main_page = 'https://www.gov.uk/government/publications/renewable-energy-planning-database-monthly-extract'\n",
    "current_link = util.helper.get_beis_link(uk_main_page)\n",
    "current_filename = current_link.split('/')[-1]\n",
    "\n",
    "source_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'url'] = current_link\n",
    "source_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'filename'] = current_filename\n",
    "source_df.to_csv(source_list_filepath, index=False, header=True)\n",
    "\n",
    "source_df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as of April 5, 2019, the following sources are available only from the OPSD server and the data will be downloaded from it even if `download_from` is set to `'original_sources'`:\n",
    "- Urzad Regulacji Energetyki (PL)\n",
    "- Energinet (DK)\n",
    "\n",
    "Also, the BNetzA PV dataset will be loaded from OPSD server. The reason is that the \"April 2018\" sheet’s date columns needed \"date\" formatting to be manually re-applied from row 5221 onwards, otherwise the some date values would have been read as integer values. Therefore: This dataset will be loaded from OPSD server as well instead of its original source:\n",
    "- Bundesnetzagentur BNetzA PV (DE): \"Meldungen_Juli17-Dez18.xlsx\"\n",
    "\n",
    "The original links which should be downloaded from OPSD are marked as inactive in the column `active` in the above dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the downloader for data sources\n",
    "The `Downloader` class in the `util` package is responsible for downloading the original files to appropriate folders. In order to access its functionality, we have to instantiate it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import util.downloader\n",
    "from util.downloader import Downloader\n",
    "downloader = Downloader(version, input_directory_path, source_list_filepath, download_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the NUTS converter\n",
    "The `NUTSConverter` class in the `util` package uses the information on each facility's  postcode, municipalty name, municipality code, longitude, and latitude to assign it correct [NUTS 2016](https://ec.europa.eu/eurostat/web/nuts/history) level 1, 2, and 3 codes.\n",
    "\n",
    "Here, we instantiate the converter so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.nuts_converter import NUTSConverter\n",
    "nuts_converter = NUTSConverter(downloader, eurostat_eu_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup translation dictionaries\n",
    "\n",
    "Column and value names of the original data sources will be translated to English and standardized across different sources. Standardized column names, e.g. \"electrical_capacity\" are required to merge data in one DataFrame.<br>\n",
    "The column and the value translation lists are provided in the input folder of the Data Package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get column translation list\n",
    "columnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\n",
    "columnnames.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value translation list\n",
    "valuenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\n",
    "valuenames.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process per country\n",
    "\n",
    "For one country after the other, the original data is downloaded, read, processed, translated, eventually georeferenced and saved. If respective files are already in the local folder, these will be utilized.\n",
    "To process the provided data [pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is applied.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Germany DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read\n",
    "The data which will be processed below is provided by the following data sources:\n",
    "\n",
    "**[Netztransparenz.de](https://www.netztransparenz.de/de/Anlagenstammdaten.htm)** - Official grid transparency platform from the German Transmission System Operators (TSOs): 50Hertz, Amprion, TenneT and TransnetBW.\n",
    "\n",
    "**[Bundesnetzagentur (BNetzA)](https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/ZahlenDatenInformationen/EEG_Registerdaten/EEG_Registerdaten_node.html)** - German Federal Network Agency for Electricity, Gas, Telecommunications, Posts and Railway (In separate files for **data for roof-mounted PV power plants** and for **all other renewable energy power plants**.)\n",
    "\n",
    "Data URL for BNetzA gets updated every few month. To be sure, always check if the links (url_bnetza; url_bnetza_pv) are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists of source names\n",
    "downloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n",
    "\n",
    "tsos = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW']\n",
    "datasets = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW','bnetza','bnetza_pv','bnetza_pv_historic']\n",
    "\n",
    "# Download the files and get the local file paths indexed by source names\n",
    "filepaths = downloader.download_data_for_country('DE')\n",
    "\n",
    "# Remove the Eurostat NUTS file as it's a geoinformation source\n",
    "DE_postcode2nuts_filepath = filepaths.pop('Eurostat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all data sets before processing.\n",
    "filenames = {}\n",
    "\n",
    "for source in filepaths:\n",
    "    filepath = filepaths[source]\n",
    "    if os.path.splitext(filepath)[1] != '.xlsx' and zipfile.is_zipfile(filepath):\n",
    "        %time filenames[source] = zipfile.ZipFile(filepath)\n",
    "    else:\n",
    "        %time filenames[source] = filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TSO data from zip file\n",
    "dfs = {}\n",
    "\n",
    "basenames_by_tso = {\n",
    "    '50Hertz': 'Netztransparenz%20Anlagenstammdaten%202017%2050Hertz%20Transmission%20GmbH',\n",
    "    'Amprion': 'Netztransparenz%20Anlagenstammdaten%202017%20Amprion%20GmbH_V03',\n",
    "    'TenneT': 'Netztransparenz%20Anlagenstammdaten%202017%20TenneT%20TSO%20GmbH',\n",
    "    'TransnetBW': 'Netztransparenz%20Anlagenstammdaten%202017%20TransnetBW%20GmbH',\n",
    "}\n",
    "\n",
    "for tso in tsos:\n",
    "    filename = urllib.parse.unquote(basenames_by_tso[tso]+'.csv')\n",
    "    print('Reading '+filename)\n",
    "    print(filenames[tso].namelist())\n",
    "    dfs[tso] = pd.read_csv(\n",
    "        filenames[tso].open(filename),\n",
    "        sep=';',\n",
    "        thousands='.',\n",
    "        decimal=',',\n",
    "\n",
    "        # Headers have to have the same order for all TSOs. Therefore just define headers here.\n",
    "        # Remove the following three lines if for next version, headers should be read out initially to then check if order is the same everywhere.\n",
    "        names=['EEG-Anlagenschlüssel','Netzbetreiber Betriebsnummer','Netzbetreiber Name','STRASSE_FLURSTUECK','PLZ','Ort / Gemarkung','Gemeindeschlüssel','Bundesland','Installierte Leistung','Energieträger','Spannungsebene','Leistungsmessung','Regelbarkeit','Inbetriebnahme','Außerbetriebnahme','Netzzugang','Netzabgang'],\n",
    "        header=None,\n",
    "        skiprows=1,\n",
    "\n",
    "        parse_dates=[11, 12, 13, 14],\n",
    "        encoding='iso-8859-1',\n",
    "        dayfirst=True,\n",
    "        low_memory=False\n",
    "    )\n",
    "    print('Done reading '+tso+'_EEG-Anlagenstammdaten_2016.csv')\n",
    "\n",
    "for filename in filenames.values():\n",
    "    if(isinstance(filename, zipfile.ZipFile)):\n",
    "        #print(filename)\n",
    "        filename.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BNetzA register\n",
    "print('Reading bnetza: '+filenames['bnetza'])\n",
    "dfs['bnetza'] = pd.read_excel(filenames['bnetza'],\n",
    "                          sheet_name='Gesamtübersicht',\n",
    "                          header=0,\n",
    "                          converters={'4.9 Postleit-zahl': str, 'Gemeinde-Schlüssel': str}\n",
    ")\n",
    "\n",
    "skiprows = {'bnetza_pv_historic': 10, 'bnetza_pv': 9}\n",
    "\n",
    "for dataset in ['bnetza_pv', 'bnetza_pv_historic']:\n",
    "    print(dataset)\n",
    "    print('Reading '+dataset+': '+filenames[dataset])\n",
    "    xls_handle = pd.ExcelFile(filenames[dataset])\n",
    "    print('Concatenating all '+dataset+' sheets into one dataframe')\n",
    "    dfs[dataset] = pd.concat(\n",
    "        (xls_handle.parse(\n",
    "            sheet,\n",
    "            skiprows=skiprows[dataset],\n",
    "            converters={'Anlage \\nPLZ': str}\n",
    "        ) for sheet in xls_handle.sheet_names),\n",
    "        sort=True\n",
    "    )\n",
    "    dfs[dataset].tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['bnetza_pv_historic'] = dfs['bnetza_pv_historic'].drop(['Unnamed: 7'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( dfs, open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pickle.load( open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names\n",
    "To standardise the DataFrame the original column names from the German TSOs and the BNetzA wil be translated and new english column names wil be assigned to the DataFrame. The unique column names are required to merge the DataFrame.<br>\n",
    "The column_translation_list is provided here as csv in the input folder. It is loaded in _2.3 Setup of translation dictionaries_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Germany, create dictionary and show dictionary\n",
    "columnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\n",
    "idx_DE = columnnames[columnnames['country'] == 'DE'].index\n",
    "column_dict_DE = columnnames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the column translation process for each original data source\n",
    "print('Translation')\n",
    "for dataset in datasets:\n",
    "    # Remove newlines and any other duplicate whitespaces in column names:\n",
    "    dfs[dataset] = dfs[dataset].rename(columns={col: re.sub(r\"\\s+\", ' ', col) for col in dfs[dataset].columns})\n",
    "    # Do column name translations\n",
    "    dfs[dataset].rename(columns=column_dict_DE, inplace=True)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add information and choose columns\n",
    "All data source names and for the BNetzA-PV data the energy source level 2 will added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data source names to the DataFrames\n",
    "for tso in tsos:\n",
    "    dfs[tso]['data_source'] = tso\n",
    "    dfs[tso]['tso'] = tso\n",
    "\n",
    "dfs['bnetza']['data_source'] = 'BNetzA'\n",
    "dfs['bnetza_pv']['data_source'] = 'BNetzA_PV'\n",
    "dfs['bnetza_pv_historic']['data_source'] = 'BNetzA_PV_historic'\n",
    "\n",
    "# Add for the BNetzA PV data the energy source level 2\n",
    "dfs['bnetza_pv']['energy_source_level_2'] = 'Photovoltaics'\n",
    "dfs['bnetza_pv_historic']['energy_source_level_2'] = 'Photovoltaics'\n",
    "\n",
    "# Correct datetime-format\n",
    "def decom_fkt(x):\n",
    "    x = str(x)\n",
    "    if x == 'nan':\n",
    "        x = ''\n",
    "    else:\n",
    "        x = x[0:10]\n",
    "    return x\n",
    "\n",
    "dfs['bnetza']['decommissioning_date'] = dfs['bnetza']['decommissioning_date'].apply(decom_fkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select those columns of the orignal data which are utilised further\n",
    "dfs['bnetza'] = dfs['bnetza'].loc[:, ('commissioning_date', 'decommissioning_date',\n",
    "                              'notification_reason', 'energy_source_level_2',\n",
    "                              'electrical_capacity_kW', 'thermal_capacity_kW',\n",
    "                              'voltage_level', 'dso', 'eeg_id', 'bnetza_id',\n",
    "                              'federal_state', 'postcode', 'municipality_code',\n",
    "                              'municipality', 'address', 'address_number',\n",
    "                              'utm_zone', 'utm_east', 'utm_north',\n",
    "                              'data_source')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets: print(dataset+':'); display(dfs[dataset].tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DataFrames\n",
    "The individual DataFrames from the TSOs (Netztransparenz.de) and BNetzA are merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames of each original source into a common DataFrame DE_renewables\n",
    "dfs_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    dfs_list.append(dfs[dataset])\n",
    "\n",
    "DE_renewables = pd.concat(dfs_list, sort=True)\n",
    "DE_renewables.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the decommissioning_column has the right dtype\n",
    "%time DE_renewables['decommissioning_date'] = DE_renewables['decommissioning_date'].apply(pd.to_datetime)\n",
    "DE_renewables['decommissioning_date'] = pd.to_datetime(DE_renewables['decommissioning_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables.reset_index(drop=True, inplace=True)\n",
    "DE_renewables.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy source level 2\n",
    "Different German terms for energy source level 2, energy source level 3, technology and voltage levels are translated and harmonized across the individual data sources. The value_translation_list is provided here as csv in the input folder. It is loaded in _2.3 Setup of translation dictionaries_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Germany, create dictionary and show dictionary\n",
    "valuenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\n",
    "idx_DE = valuenames[valuenames['country'] == 'DE'].index\n",
    "value_dict_DE = valuenames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('replacing..')\n",
    "# Replace all original value names by the OPSD value names. \n",
    "# Running time: some minutes. %time prints the time your computer required for this step\n",
    "%time DE_renewables.replace(value_dict_DE, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables['postcode'] = DE_renewables['postcode'].apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate and assign energy source level 1 - 3 and technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary in order to assign energy_source to its subtype\n",
    "energy_source_dict_DE = valuenames.loc[idx_DE].set_index(\n",
    "    'opsd_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "# Column energy_source partly contains energy source level 3 and technology information,\n",
    "# thus this column is copied to new column technology...\n",
    "DE_renewables['technology'] = DE_renewables['energy_source_level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and the energy source level 2 values are replaced by the higher level classification\n",
    "DE_renewables['energy_source_level_2'].replace(\n",
    "    energy_source_dict_DE, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose energy source level 2 entries where energy_source is \"Bioenergy\" in order to \n",
    "# seperate Bioenergy subtypes to \"energy_source_level_3\" and subtypes for the rest to \"technology\"\n",
    "idx_DE_Bioenergy = DE_renewables[DE_renewables['energy_source_level_2'] == 'Bioenergy'].index\n",
    "\n",
    "# Assign technology to energy source level 3 for all entries where energy source level 2 is \n",
    "# Bioenergy and delete those entries from technology\n",
    "DE_renewables[['energy_source_level_3']] = DE_renewables.iloc[idx_DE_Bioenergy][['technology']]\n",
    "DE_renewables.loc[idx_DE_Bioenergy]['technology'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign energy source level 1 to the dataframe\n",
    "DE_renewables['energy_source_level_1'] = 'Renewable energy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electrical capacity per energy source level 2 (in MW)\n",
    "DE_renewables.groupby(['energy_source_level_2'])['electrical_capacity_kW'].sum() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform electrical capacity from kW to MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kW to MW\n",
    "DE_renewables[['electrical_capacity_kW', 'thermal_capacity_kW']] /= 1000\n",
    "\n",
    "# adapt column name\n",
    "DE_renewables.rename(columns={'electrical_capacity_kW': 'electrical_capacity',\n",
    "                              'thermal_capacity_kW': 'thermal_capacity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get coordinates by postcode\n",
    "*(for data with no existing geocoordinates)*\n",
    "\n",
    "The available post code in the original data provides a first approximation for the geocoordinates of the RE power plants.<br>\n",
    "The BNetzA data provides the full zip code whereas due to data privacy the TSOs only report the first three digits of the power plant's post code (e.g. 024xx) and no address. Subsequently a centroid of the post code region polygon is used to find the coordinates.\n",
    "\n",
    "With data from\n",
    "*  http://www.suche-postleitzahl.org/downloads?download=plz-gebiete.shp.zip\n",
    "*  http://www.suche-postleitzahl.org/downloads?download_file=plz-3stellig.shp.zip\n",
    "*  http://www.suche-postleitzahl.org/downloads\n",
    "\n",
    "a CSV-file for all existing German post codes with matching geocoordinates has been compiled. The latitude and longitude coordinates were generated by running a PostgreSQL + PostGIS database. Additionally the respective TSO has been added to each post code. *(A Link to the SQL script will follow here later)*\n",
    "\n",
    "*(License: http://www.suche-postleitzahl.org/downloads, Open Database Licence for free use. Source of data: © OpenStreetMap contributors)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read generated postcode/location file\n",
    "postcode = pd.read_csv(os.path.join('input', 'de_tso_postcode_full.csv'))\n",
    "\n",
    "# Drop possible duplicates in postcodes\n",
    "postcode.drop_duplicates('postcode', keep='last', inplace=True)\n",
    "\n",
    "# Show first entries\n",
    "postcode.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge geometry information by using the postcode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take postcode and longitude/latitude information\n",
    "postcode = postcode[['postcode', 'lon', 'lat']]\n",
    "\n",
    "# Cast DE_renewables['postcode'] to int64 in order to do the natural join of the dataframes\n",
    "DE_renewables['postcode'] = pd.to_numeric(DE_renewables['postcode'], errors='coerce')\n",
    "\n",
    "# Join two dataframes\n",
    "DE_renewables = DE_renewables.merge(postcode, on=['postcode'],  how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform geoinformation\n",
    "*(for data with already existing geoinformation)*\n",
    "\n",
    "In this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation. \n",
    "\n",
    "The BNetzA data set offers UTM Geoinformation with the columns *utm_zone (UTM-Zonenwert)*, *utm_east* and *utm_north*. Most of utm_east-values include the utm_zone-value **32** at the beginning of the number. In order to properly standardize and transform this geoinformation into latitude and longitude it is necessary to remove this utm_zone value. For all UTM entries the utm_zone 32 is used by the BNetzA.\n",
    "\n",
    "\n",
    "|utm_zone|\t utm_east|\t utm_north| comment|\n",
    "|---|---|---| ----|\n",
    "|32|\t413151.72|\t6027467.73| proper coordinates|\n",
    "|32|\t**32**912159.6008|\t5692423.9664| caused error by 32|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many different utm_zone values are in the data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables.groupby(['utm_zone'])['utm_zone'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the utm_zone \"32\" from the utm_east value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entries with 32 value at the beginning\n",
    "idx_32 = (DE_renewables['utm_east'].astype(str).str[:2] == '32')\n",
    "idx_notnull = DE_renewables['utm_east'].notnull()\n",
    "\n",
    "# Remove 32 from utm_east entries\n",
    "DE_renewables.loc[idx_32, 'utm_east'] = DE_renewables.loc[idx_32,\n",
    "                                                          'utm_east'].astype(str).str[2:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_latlon(utm_east, utm_north, utm_zone):\n",
    "    try:\n",
    "        return utm.to_latlon(utm_east, utm_north, utm_zone, 'U')\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "DE_renewables['latlon'] = DE_renewables.loc[idx_notnull, ['utm_east', 'utm_north', 'utm_zone']].apply(\n",
    "        lambda x: convert_to_latlon(x[0], x[1], x[2]), axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion UTM to latitude and longitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for row in DE_renewables['latlon']:\n",
    "    try:\n",
    "        # Split tuple format into the column lat and lon\n",
    "        row = row.lstrip('(').rstrip(')')\n",
    "        parts = row.split(',')\n",
    "        if(len(parts)<2):\n",
    "            raise Exception('This is not a proper tuple. So go to exception block.')\n",
    "        lat.append(parts[0])\n",
    "        lon.append(parts[1])\n",
    "    except:\n",
    "        # set NaN\n",
    "        lat.append(np.NaN)\n",
    "        lon.append(np.NaN)\n",
    "\n",
    "DE_renewables['latitude'] = pd.to_numeric(lat)\n",
    "DE_renewables['longitude'] = pd.to_numeric(lon)\n",
    "\n",
    "# Add new values to DataFrame lon and lat\n",
    "DE_renewables['lat'] = DE_renewables[['lat', 'latitude']].apply(\n",
    "    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n",
    "    axis=1)\n",
    "\n",
    "DE_renewables['lon'] = DE_renewables[['lon', 'longitude']].apply(\n",
    "    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check: missing coordinates by data source and type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DE_renewables[DE_renewables['data_source'] == '50Hertz'].to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Missing coordinates ', DE_renewables.lat.isnull().sum())\n",
    "\n",
    "display(\n",
    "    DE_renewables[DE_renewables.lat.isnull()].groupby(\n",
    "        ['energy_source_level_2','data_source']\n",
    "    )['data_source'].count()\n",
    ")\n",
    "\n",
    "print('Share of missing coordinates (note that NaN can mean it\\'s all fine):')\n",
    "\n",
    "DE_renewables[DE_renewables.lat.isnull()].groupby(\n",
    "        ['energy_source_level_2','data_source']\n",
    "    )['data_source'].count() / DE_renewables.groupby(\n",
    "        ['energy_source_level_2','data_source']\n",
    "    )['data_source'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove temporary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lonlat column that contains both, latitute and longitude\n",
    "DE_renewables.drop(['latlon', 'longitude', 'latitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    DE_renewables[DE_renewables['data_source'] == 'TenneT'].head(3)[['lon','lat','data_source']]\n",
    ")\n",
    "display(\n",
    "    DE_renewables[DE_renewables['data_source'] == 'BNetzA'].head(3)[['lon','lat','data_source']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save temporary Pickle (to have a point to quickly return to if things break after this point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( DE_renewables, open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables = pickle.load( open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove out-of-range dates\n",
    "# Keep only values between 1900 and 2100 to rule out outliers / wrong values. Also, Excel doesn't support < 1900 entries.\n",
    "\n",
    "DE_renewables = DE_renewables[\n",
    "    (DE_renewables['commissioning_date']>pd.Timestamp('1900')) & \n",
    "    (DE_renewables['commissioning_date']<pd.Timestamp('2107'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables['municipality_code'] = DE_renewables['municipality_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from municipality code\n",
    "DE_renewables['municipality_code'] = DE_renewables['municipality_code'].str.replace(' ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables['municipality_code'] = pd.to_numeric(DE_renewables['municipality_code'], errors='coerce', downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge address and address_number\n",
    "to_string = lambda x: str(x) if not pd.isnull(x) else ''\n",
    "DE_renewables['address'] = DE_renewables['address'].map(to_string) + ' ' + DE_renewables['address_number'].map(to_string)\n",
    "\n",
    "# Make sure that the column has no whitespaces at the beginning and the end\n",
    "DE_renewables['address'] = DE_renewables['address'].str.strip()\n",
    "\n",
    "# Remove the column with address numbers as it is not needed anymore\n",
    "del DE_renewables['address_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign NUTS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a temporary postcode column as a string column for joining with the appropriate NUTS correspondence table\n",
    "DE_renewables['postcode_str'] = DE_renewables['postcode'].astype(str).str[:-2]\n",
    "\n",
    "DE_renewables = nuts_converter.add_nuts_information(DE_renewables, 'DE', DE_postcode2nuts_filepath,\n",
    "                                                     postcode_column='postcode_str',\n",
    "                                                     how=['postcode', 'municipality_code', 'municipality', 'latlon'])\n",
    "\n",
    "# Drop the temporary column\n",
    "DE_renewables.drop('postcode_str', axis='columns', inplace=True)\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = DE_renewables['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = DE_renewables['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(DE_renewables['lat'],\n",
    "                 DE_renewables['lon'],\n",
    "                'Germany',\n",
    "                 categories=DE_renewables['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save\n",
    " \n",
    "The merged, translated, cleaned, DataFrame will be saved temporily as a pickle file, which stores a Python object fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_renewables.to_pickle('intermediate/DE_renewables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del DE_renewables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denmark DK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read\n",
    "The data which will be processed below is provided by the following data sources:\n",
    "\n",
    "**[Energistyrelsen (ens) / Danish Energy Agency](http://www.ens.dk/info/tal-kort/statistik-noegletal/oversigt-energisektoren/stamdataregister-vindmoller)** - The wind turbines register is released by the Danish Energy Agency. \n",
    "\n",
    "**[Energinet.dk](http://www.energinet.dk/DA/El/Engrosmarked/Udtraek-af-markedsdata/Sider/Statistik.aspx)** - The data of solar power plants are released by the leading transmission network operator Denmark.\n",
    "\n",
    "**[geonames.org](http://download.geonames.org/export/zip/?C=N;O=D)** - The postcode  data from Denmark is provided by Geonames and licensed under a [Creative Commons Attribution 3.0 license](http://creativecommons.org/licenses/by/3.0/).\n",
    "\n",
    "**[Eurostat](https://ec.europa.eu/eurostat/about/overview)** - The data for converting information on municipalities, postcodes and geographic coordinates to NUTS 2016 classification codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data for Denmark\n",
    "filepaths = downloader.download_data_for_country('DK')\n",
    "print(filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for reading the data on the wind turbines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dk_wind_turbines(filepath, sheet_name):\n",
    "    # Reads the data on Danish wind turbines\n",
    "    # from the sheet of the given name\n",
    "    # in the file with the path.\n",
    "    # Returns the data as a Pandas dataframe.\n",
    "    \n",
    "    book = xlrd.open_workbook(filepath)\n",
    "    sheet = book.sheet_by_name(sheet_name)\n",
    "    \n",
    "    # Since the column names are in two rows, not one,\n",
    "    # collect them in two parts. The first part is\n",
    "    # fixed and contains column names.\n",
    "    header = []\n",
    "    for i in range(0, 16):\n",
    "        # Make sure that strings 1) do not contain the newline sign\n",
    "        # and 2) have no trailing blank spaces.\n",
    "        column_name = sheet.cell_value(17, i).replace(\"\\n\", \"\").strip()\n",
    "        header = header + [column_name]\n",
    "    # The second part is variable. It consists of two subparts:\n",
    "    # 1) previous years (type float)\n",
    "    # 2) the past months of the current year (type date)\n",
    "    \n",
    "    # Reading previous years as column names\n",
    "    i = 16\n",
    "    cell = sheet.cell(16, i)\n",
    "\n",
    "    while cell.ctype == xlrd.XL_CELL_NUMBER:\n",
    "        column_name = str(int(cell.value))\n",
    "        header = header + [column_name]\n",
    "        i = i + 1\n",
    "        cell = sheet.cell(16, i)\n",
    "    \n",
    "    # Reading the months of the current year as column names\n",
    "    while cell.ctype == xlrd.XL_CELL_DATE:\n",
    "        year, month, _, _, _, _ = xlrd.xldate_as_tuple(cell.value, book.datemode)\n",
    "        column_name = str(\"{}-{}\".format(year, month))\n",
    "        header = header + [column_name]\n",
    "        i = i + 1\n",
    "        cell = sheet.cell(16, i)\n",
    "        \n",
    "    # Add the final column for the total of the current year\n",
    "    header += ['{}-total'.format(header[-1].split('-')[0])]\n",
    "        \n",
    "        \n",
    "    # Skip the first 17 rows in the sheet. The rest contains the data.\n",
    "    df = pd.read_excel(filepath,\n",
    "                       sheet_name=sheet_name,\n",
    "                       skiprows=17,\n",
    "                       skipfooter=3\n",
    "                    )\n",
    "    \n",
    "    # \n",
    "    #df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n",
    "    \n",
    "    # Set the column names.\n",
    "    df.columns = header\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get wind turbines data\n",
    "wind_turbines_sheet_name = 'IkkeAfmeldte-Existing turbines'\n",
    "DK_wind_filepath = filepaths['Energistyrelsen']\n",
    "DK_wind_df = read_dk_wind_turbines(DK_wind_filepath,\n",
    "                                   wind_turbines_sheet_name\n",
    "                                  )\n",
    "\n",
    "# Get photovoltaic data\n",
    "DK_solar_filepath = filepaths['Energinet']\n",
    "DK_solar_df = pd.read_excel(DK_solar_filepath,\n",
    "                            sheet_name='Data',\n",
    "                            skiprows=[0],\n",
    "                            converters={'Postnr': str}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose the translation terms for Denmark, create dictionary and show dictionary\n",
    "idx_DK = columnnames[columnnames['country'] == 'DK'].index\n",
    "column_dict_DK = columnnames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()\n",
    "\n",
    "# Windows has problems reading the csv entry for east and north (DK).\n",
    "# The reason might be the difference when opening the csv between linux and\n",
    "# windows.\n",
    "column_dict_DK_temp = {}\n",
    "for k, v in column_dict_DK.items():\n",
    "    column_dict_DK_temp[k] = v\n",
    "    if v == 'utm_east' or v == 'utm_north':\n",
    "        # merge 2 lines to 1\n",
    "        new_key = ''.join(k.splitlines())\n",
    "        column_dict_DK_temp[new_key] = v\n",
    "\n",
    "column_dict_DK = column_dict_DK_temp\n",
    "\n",
    "column_dict_DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace column names based on column_dict_DK\n",
    "DK_wind_df.rename(columns=column_dict_DK, inplace=True)\n",
    "DK_solar_df.rename(columns=column_dict_DK, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source and missing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add names of the data sources to the DataFrames\n",
    "DK_wind_df['data_source'] = 'Energistyrelsen'\n",
    "DK_solar_df['data_source'] = 'Energinet.dk'\n",
    "\n",
    "# Add energy source level 2 and technology for each of the two DataFrames\n",
    "DK_wind_df['energy_source_level_2'] = 'Wind'\n",
    "DK_solar_df['energy_source_level_2'] = 'Solar'\n",
    "DK_solar_df['technology'] = 'Photovoltaics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy source level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Denmark, create dictionary and show dictionary\n",
    "idx_DK = valuenames[valuenames['country'] == 'DK'].index\n",
    "value_dict_DK = valuenames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all original value names by the OPSD value names\n",
    "DK_wind_df.replace(value_dict_DK, inplace=True)\n",
    "DK_solar_df.replace(value_dict_DK, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UTM32 to latitude and longitude** *(Data from Energistyrelsen)*\n",
    "\n",
    "The Energistyrelsen data set offers UTM Geoinformation with the columns utm_east and utm_north belonging to the UTM zone 32. In this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for all values with utm information\n",
    "idx_notnull = DK_wind_df['utm_east'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from UTM values to latitude and longitude coordinates\n",
    "DK_wind_df['lonlat'] = DK_wind_df.loc[idx_notnull, ['utm_east', 'utm_north']\n",
    "                                      ].apply(lambda x: utm.to_latlon(x[0],\n",
    "                                                                      x[1],\n",
    "                                                                      32,\n",
    "                                                                      'U'), axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split latitude and longitude in two columns\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for row in DK_wind_df['lonlat']:\n",
    "    try:\n",
    "        # Split tuple format\n",
    "        # into the column lat and lon\n",
    "        row = row.lstrip('(').rstrip(')')\n",
    "        lat.append(row.split(',')[0])\n",
    "        lon.append(row.split(',')[1])\n",
    "    except:\n",
    "        # set NAN\n",
    "        lat.append(np.NaN)\n",
    "        lon.append(np.NaN)\n",
    "\n",
    "DK_wind_df['lat'] = pd.to_numeric(lat)\n",
    "DK_wind_df['lon'] = pd.to_numeric(lon)\n",
    "\n",
    "# drop lonlat column that contains both, latitute and longitude\n",
    "DK_wind_df.drop('lonlat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Postcode to lat/lon (WGS84)**\n",
    "*(for data from Energinet.dk)*\n",
    "\n",
    "The available post code in the original data provides an approximation for the geocoordinates of the solar power plants.<br>\n",
    "The postcode will be assigned to latitude and longitude coordinates with the help of the postcode table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geo-information\n",
    "zip_DK_geo = zipfile.ZipFile(filepaths['Geonames'])\n",
    "\n",
    "# Read generated postcode/location file\n",
    "DK_geo = pd.read_csv(zip_DK_geo.open('DK.txt'), sep='\\t', header=-1)\n",
    "\n",
    "# add column names as defined in associated readme file\n",
    "DK_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n",
    "                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n",
    "                  'admin_code3', 'lat', 'lon', 'accuracy']\n",
    "\n",
    "# Drop rows of possible duplicate postal_code\n",
    "DK_geo.drop_duplicates('postcode', keep='last', inplace=True)\n",
    "DK_geo['postcode'] = DK_geo['postcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add longitude/latitude infomation assigned by postcode (for Energinet.dk data)\n",
    "DK_solar_df = DK_solar_df.merge(DK_geo[['postcode', 'lon', 'lat']],\n",
    "                                on=['postcode'],\n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of units with missing coordinates separated by wind and solar\n",
    "print('Missing Coordinates DK_wind', DK_wind_df.lat.isnull().sum(), 'out of', len(DK_wind_df.index))\n",
    "print('Missing Coordinates DK_solar', DK_solar_df.lat.isnull().sum(), 'out of', len(DK_solar_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_DK_geo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DataFrames, add NUTS information and choose columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames for wind and solar into DK_renewables\n",
    "dataframes = [DK_wind_df, DK_solar_df]\n",
    "DK_renewables = pd.concat(dataframes, sort=False)\n",
    "DK_renewables = DK_renewables.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign energy source level 1 to the dataframe\n",
    "DK_renewables['energy_source_level_1'] = 'Renewable energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the address and address-number columns into one\n",
    "to_string = lambda x: str(x) if not pd.isnull(x) else \"\"\n",
    "DK_renewables['address'] = DK_renewables['address'].map(to_string) + \" \" + DK_renewables['address_number'].map(to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the column has no whitespaces at the beginning or the end\n",
    "DK_renewables['address'] = DK_renewables['address'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign NUTS codes\n",
    "DK_postcode2nuts = filepaths['Eurostat']\n",
    "DK_renewables = nuts_converter.add_nuts_information(DK_renewables, 'DK', DK_postcode2nuts,\n",
    "                                                    how=['latlon', 'postcode', 'municipality_code', 'municipality_name'])\n",
    "\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = DK_renewables['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = DK_renewables['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select those columns of the orignal data which are utilised further\n",
    "columns_of_interest = ['commissioning_date', 'energy_source_level_1', 'energy_source_level_2',\n",
    "                   'technology', 'electrical_capacity_kW', 'dso', 'gsrn_id', 'postcode',\n",
    "                   'municipality_code', 'municipality', 'address',\n",
    "                   'utm_east', 'utm_north', 'lon', 'lat', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n",
    "                   'hub_height', 'rotor_diameter', 'manufacturer', 'model', 'data_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean DataFrame from columns other than specified above\n",
    "DK_renewables = DK_renewables.loc[:, columns_of_interest]\n",
    "DK_renewables.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform electrical_capacity from kW to MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kW to MW\n",
    "DK_renewables['electrical_capacity_kW'] /= 1000\n",
    "\n",
    "# adapt column name\n",
    "DK_renewables.rename(columns={'electrical_capacity_kW': 'electrical_capacity'},\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_points(DK_renewables['lat'],\n",
    "                 DK_renewables['lon'],\n",
    "                 'Denmark',\n",
    "                 categories=DK_renewables['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DK_renewables.to_pickle('intermediate/DK_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## France FR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read\n",
    "The data which will be processed below is provided by the following data sources:\n",
    "\n",
    "**[Ministry for the Ecological and Inclusive Transition](https://www.statistiques.developpement-durable.gouv.fr/donnees-locales-relatives-aux-installations-de-production-delectricite-renouvelable-beneficiant-0?rubrique=23&dossier=189)** - Number of installations and installed capacity of the different renewable source for every municipality in France. Service of observation and statistics, survey, date of last update: 31/01/2019. Data until 31/12/2017.\n",
    "\n",
    "The available INSEE code in the original data provides a first approximation for the geocoordinates of the renewable power plants. The following data source is utilized for assigning INSEE code to coordinates of the municipalities:\n",
    "\n",
    "**[OpenDataSoft](http://public.opendatasoft.com/explore/dataset/code-postal-code-insee-2015/information/)** - a list of French INSEE codes and corresponding coordinates, published under the [Licence Ouverte (Etalab)](https://www.etalab.gouv.fr/licence-ouverte-open-licence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "filepaths = downloader.download_data_for_country('FR')\n",
    "print(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of renewables per municipality\n",
    "FR_re_filepath = filepaths['gouv.fr']\n",
    "\n",
    "FR_re_df = pd.read_excel(FR_re_filepath,\n",
    "                         sheet_name='Commune',\n",
    "                         encoding='UTF8',\n",
    "                         thousands='.',\n",
    "                         decimals=',',\n",
    "                         header=[3, 4],\n",
    "                         skipfooter=8,  # contains summarized values\n",
    "                         index_col=[0, 1],  # required for MultiIndex\n",
    "                         converters={'Code officiel géographique': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange columns and translate column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The French data source contains number of installations and sum of installed capacity per energy source per municipality. The structure is adapted to the power plant list of other countries. The list is limited to the plants which are covered by article 10 of february 2000 by an agreement to a purchase commitment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange data\n",
    "FR_re_df.index.rename(['insee_com', 'municipality'], inplace=True)\n",
    "FR_re_df.columns.rename(['energy_source_level_2', None], inplace=True)\n",
    "FR_re_df = (FR_re_df\n",
    "            .stack(level='energy_source_level_2', dropna=False)\n",
    "            .reset_index(drop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for France, create dictionary and show dictionary\n",
    "idx_FR = columnnames[columnnames['country'] == 'FR'].index\n",
    "column_dict_FR = columnnames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\n",
    "column_dict_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate columnnames\n",
    "FR_re_df.rename(columns=column_dict_FR, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that contain NA \n",
    "FR_re_df = FR_re_df.dropna()\n",
    "FR_re_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_re_df['data_source'] = 'Ministry for the Ecological and Inclusive Transition'\n",
    "FR_re_df['as_of_year'] = 2017 # Year for which the dataset has been compiled by the data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy source level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kept secret if number of installations < 3**\n",
    "\n",
    "If the number of installations is less than 3, it is marked with an _s_ instead of the number 1 or 2 due to statistical confidentiality ([further explanation by the data provider](http://www.statistiques.developpement-durable.gouv.fr/fileadmin/documents/Themes/Energies_et_climat/Les_differentes_energies/Energies_renouvelables/donnees_locales/2014/methodo-donnees-locales-electricte-renouvelable-12-2015-b.pdf)). Here, the _s_ is changed to _< 3_. This is done in the same step as the other value translations of the energy sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for France, create dictionary and show dictionary\n",
    "idx_FR = valuenames[valuenames['country'] == 'FR'].index\n",
    "value_dict_FR = valuenames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\n",
    "value_dict_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all original value names by the OPSD value names\n",
    "FR_re_df.replace(value_dict_FR, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate and assign energy source level 1-3 and technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary in order to assign energy_source to its subtype\n",
    "energy_source_dict_FR = valuenames.loc[idx_FR].set_index(\n",
    "    'opsd_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "# Column energy_source partly contains subtype information, thus this column is copied\n",
    "# to new column for energy_source_subtype...\n",
    "FR_re_df['technology'] = FR_re_df['energy_source_level_2']\n",
    "\n",
    "# ...and the energy source subtype values in the energy_source column are replaced by\n",
    "# the higher level classification\n",
    "FR_re_df['energy_source_level_2'].replace(energy_source_dict_FR, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign energy_source_level_1 to the dataframe\n",
    "FR_re_df['energy_source_level_1'] = 'Renewable energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_re_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose energy source level 2 entries where energy source level 2 is Bioenergy in order to  \n",
    "# seperate Bioenergy subtypes to energy source level 3 and subtypes for the rest to technology\n",
    "idx_FR_Bioenergy = FR_re_df[FR_re_df['energy_source_level_2'] == 'Bioenergy'].index\n",
    "\n",
    "# Assign technology to energy source level 3  for all entries where energy source level 2 is  \n",
    "# Bioenergy and delete those entries from  technology\n",
    "FR_re_df[['energy_source_level_3']] = FR_re_df.iloc[idx_FR_Bioenergy][['technology']]\n",
    "FR_re_df.loc[idx_FR_Bioenergy,'technology'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Municipality (INSEE) code to lon/lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the local path of the downloaded georeferencing data\n",
    "FR_geo_filepath = filepaths['Opendatasoft']\n",
    "\n",
    "# Read INSEE Code Data\n",
    "FR_geo = pd.read_csv(FR_geo_filepath,\n",
    "                     sep=';',\n",
    "                     header=0,\n",
    "                     converters={'Code_postal': str})\n",
    "\n",
    "# Drop possible duplicates of the same INSEE code\n",
    "FR_geo.drop_duplicates('INSEE_COM', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for latitude/longitude\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "# split in latitude/longitude\n",
    "for row in FR_geo['Geo Point']:\n",
    "    try:\n",
    "        # Split tuple format\n",
    "        # into the column lat and lon\n",
    "        row = row.lstrip('(').rstrip(')')\n",
    "        lat.append(row.split(',')[0])\n",
    "        lon.append(row.split(',')[1])\n",
    "    except:\n",
    "        # set NAN\n",
    "        lat.append(np.NaN)\n",
    "        lon.append(np.NaN)\n",
    "\n",
    "# add these columns to the INSEE DataFrame\n",
    "FR_geo['lat'] = pd.to_numeric(lat)\n",
    "FR_geo['lon'] = pd.to_numeric(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names of merge key have to be named identically\n",
    "FR_re_df.rename(columns={'municipality_code': 'INSEE_COM'}, inplace=True)\n",
    "\n",
    "# Merge longitude and latitude columns by the Code INSEE\n",
    "FR_re_df = FR_re_df.merge(FR_geo[['INSEE_COM', 'lat', 'lon']],\n",
    "                          on=['INSEE_COM'],\n",
    "                          how='left')\n",
    "\n",
    "# Translate Code INSEE column back to municipality_code\n",
    "FR_re_df.rename(columns={'INSEE_COM': 'municipality_code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine NUTS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_postcode2nuts_path = filepaths['Eurostat']\n",
    "\n",
    "FR_re_df = nuts_converter.add_nuts_information(FR_re_df, 'FR', FR_postcode2nuts_path,\n",
    "                                               how=['municipality_code', 'latlon'])\n",
    "# how=['municipality', 'municipality_code', 'latlon']\n",
    "# Report the number of facilites whose NUTS codes were successfully determined\n",
    "determined = FR_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', FR_re_df.shape[0], 'facilities in FR.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = FR_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', FR_re_df.shape[0], 'facilities in FR.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the facilities without NUTS classification\n",
    "FR_re_df[FR_re_df['nuts_1_region'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the facilities for which we could not determine NUTS codes are located in the municipality of Tsingoni, a commune in the French overseas department of Mayotte, in the Indian Ocean, which does not fall under NUTS classification because it is not located in Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(FR_re_df['lat'],\n",
    "                 FR_re_df['lon'],\n",
    "                 'France',\n",
    "                 categories=FR_re_df['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_re_df.to_pickle('intermediate/FR_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poland PL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read\n",
    "The data which will be processed below was provided by the following data source:\n",
    "\n",
    "**[Urzad Regulacji Energetyki (URE) / Energy Regulatory Office](http://www.ure.gov.pl/uremapoze/mapa.html)** - Number of installations and installed capacity per energy source of renewable energy. Summed per powiat (districts).\n",
    "\n",
    "However, the data are no longer available from the original source, so they can be downloaded only from the OPSD server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available only from the OPSD server\n",
    "filepaths = downloader.download_data_for_country('PL')\n",
    "PL_filepath = filepaths['Urzad Regulacji Energetyki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read rtf-file to string with the correct encoding\n",
    "with open(PL_filepath, 'r') as rtf:\n",
    "    file_content = rtf.read()\n",
    "\n",
    "file_content = file_content.encode('utf-8').decode('iso-8859-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange data from rft-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rtf file has one table for each district in the rtf-file which needs to be separated from each and other and restructured to get all plants in one DataFrame with the information: district, energy_source, number_of_installations, installed_capacity. Thus in the following, the separating items are defined, the district tables split in parts, all put in one list and afterwards transferred to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new line is separating all parts\n",
    "sep_split_into_parts = r'{\\fs12 \\f1 \\line }'\n",
    "# separates the table rows of each table\n",
    "sep_data_parts = r'\\trql'\n",
    "\n",
    "reg_exp_district = r'(?<=Powiat:).*(?=})'\n",
    "\n",
    "reg_exp_installation_type = (\n",
    "    r'(?<=\\\\fs12 \\\\f1 \\\\pard \\\\intbl \\\\ql \\\\cbpat[2|3|4] \\{\\\\fs12 \\\\f1  ).*(?=\\})')\n",
    "reg_exp_installation_value = (\n",
    "    r'(?<=\\\\fs12 \\\\f1 \\\\pard \\\\intbl \\\\qr \\\\cbpat[3|4] \\{\\\\fs12 \\\\f1 ).*(?=})')\n",
    "\n",
    "# split file into parts\n",
    "parts = file_content.split(sep_split_into_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing the data\n",
    "data_set = []\n",
    "for part in parts:\n",
    "    # match district\n",
    "    district = re.findall(reg_exp_district, part)\n",
    "\n",
    "    if len(district) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        district = district[0].lstrip()\n",
    "        # separate each part\n",
    "        data_parts = part.split(sep_data_parts)\n",
    "        # data structure:\n",
    "        # data_row = {'district': '', 'install_type': '', 'quantity': '', 'power': ''}\n",
    "        for data_rows in data_parts:\n",
    "            if \"SUM\" in data_rows:\n",
    "                # The summary follows now and we should omit it\n",
    "                break\n",
    "            wrapper_list = []\n",
    "            # match each installation type\n",
    "            installation_type = re.findall(reg_exp_installation_type, data_rows)\n",
    "            for inst_type in installation_type:\n",
    "                wrapper_list.append({'district': district, 'technology': inst_type})\n",
    "            # match data - contains twice as many entries as\n",
    "            # installation type (quantity, power vs. install type)\n",
    "            data_values = re.findall(reg_exp_installation_value, data_rows)\n",
    "            if len(data_values) == 0:\n",
    "                #log.debug('data values empty')\n",
    "                pass\n",
    "            else:\n",
    "                # connect data\n",
    "                for i, _ in enumerate(wrapper_list):\n",
    "                    wrapper_list[i]['number_of_installations'] = data_values[(\n",
    "                        i * 2)]\n",
    "                    wrapper_list[i]['electrical_capacity'] = data_values[(\n",
    "                        i * 2) + 1]\n",
    "\n",
    "                # prepare to write to file\n",
    "                for data in wrapper_list:\n",
    "                    data_set.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of malformed unicode which appear in the Polish district names\n",
    "polish_truncated_unicode_map = {\n",
    "    r'\\uc0\\u322': 'ł',\n",
    "    r'\\uc0\\u380': 'ż',\n",
    "    r'\\uc0\\u243': 'ó',\n",
    "    r'\\uc0\\u347': 'ś',\n",
    "    r'\\uc0\\u324': 'ń',\n",
    "    r'\\uc0\\u261': 'ą',\n",
    "    r'\\uc0\\u281': 'ę',\n",
    "    r'\\uc0\\u263': 'ć',\n",
    "    r'\\uc0\\u321': 'Ł',\n",
    "    r'\\uc0\\u378': 'ź',\n",
    "    r'\\uc0\\u346': 'Ś',\n",
    "    r'\\uc0\\u379': 'Ż'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing malformed unicode\n",
    "for entry in data_set:\n",
    "    while r'\\u' in entry['district']:\n",
    "        index = entry['district'].index(r'\\u')\n",
    "        offset = index + 9\n",
    "        to_be_replaced = entry['district'][index:offset]\n",
    "        if to_be_replaced in polish_truncated_unicode_map.keys():\n",
    "            # offset + 1 because there is a trailing whitespace\n",
    "            entry['district'] = entry['district'].replace(entry['district'][index:offset + 1],\n",
    "                                                          polish_truncated_unicode_map[to_be_replaced])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame with similar structure as the other countries\n",
    "PL_re_df = pd.DataFrame(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL_re_df['data_source'] = 'Urzad Regulacji Energetyki'\n",
    "PL_re_df['as_of_year'] = 2016 # Year for which the dataset has been compiled by the data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy source levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for France, create dictionary and show dictionary\n",
    "idx_PL = valuenames[valuenames['country'] == 'PL'].index\n",
    "value_dict_PL = valuenames.loc[idx_PL].set_index('original_name')['opsd_name'].to_dict()\n",
    "value_dict_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all original value names by the OPSD value names\n",
    "PL_re_df.technology.replace(value_dict_PL, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign energy source level 1-3 and technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign energy_source_level_1 to the dataframe\n",
    "PL_re_df['energy_source_level_1'] = 'Renewable energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary in order to assign energy_source to its subtype\n",
    "energy_source_dict_PL = valuenames.loc[idx_PL].set_index(\n",
    "    'opsd_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "# Create new column for energy_source\n",
    "PL_re_df['energy_source_level_2'] = PL_re_df.technology\n",
    "\n",
    "# Fill this with the energy source instead of subtype information\n",
    "PL_re_df.energy_source_level_2.replace(energy_source_dict_PL, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose energy_source entries where energy_source is \"Bioenergy\" in order to  \n",
    "# seperate Bioenergy subtypes to \"energy_source_level_3\" and subtypes for the rest to \"technology\"\n",
    "idx_PL_Bioenergy = PL_re_df[PL_re_df['energy_source_level_2'] == 'Bioenergy'].index\n",
    "\n",
    "# Assign technology to \"energy_source_level_3\" for all entries where energy_source_level_2 is\n",
    "# \"Bioenergy\" and delete those entries from \"technology\"\n",
    "PL_re_df[['energy_source_level_3']] = PL_re_df.iloc[idx_PL_Bioenergy][['technology']]\n",
    "PL_re_df.loc[idx_PL_Bioenergy, 'technology'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjust datatype of numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change type to numeric\n",
    "PL_re_df['electrical_capacity'] = pd.to_numeric(PL_re_df['electrical_capacity'])\n",
    "# Additionally commas are deleted\n",
    "PL_re_df['number_of_installations'] = pd.to_numeric(\n",
    "    PL_re_df['number_of_installations'].str.replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate and assign energy source level 1**\n",
    "\n",
    "For entries/rows of the same district and energy source level 2, electrical capacity and number of installations are aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this step.\n",
    "#PL_re_df = PL_re_df.groupby(['district', 'energy_source_level_2', 'technology'],\n",
    "#                            as_index=False\n",
    "#                            ).agg({'electrical_capacity': sum,\n",
    "#                                   'number_of_installations': sum,\n",
    "#                                   'data_source': 'first'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign NUTS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL_postcode2nuts_path = filepaths['Eurostat']\n",
    "\n",
    "PL_re_df = nuts_converter.add_nuts_information(PL_re_df, 'PL', PL_postcode2nuts_path,\n",
    "                                               municipality_column='district', how=['municipality'])\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = PL_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', PL_re_df.shape[0], 'facilities in PL.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = PL_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', PL_re_df.shape[0], 'facilities in PL.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the districts for which NUTS classification could not be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL_re_df[PL_re_df['nuts_1_region'].isnull()]['district'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of those districts are in possessive forms, whereas LAU (local administrative units) in Eurostat tables are not (e.g. `bolesławiecki` vs `Bolesław`), which causes mismatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL_re_df.to_pickle('intermediate/PL_renewables.pickle')\n",
    "PL_re_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switzerland CH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read\n",
    "The data which will be processed below is provided by the following data sources:\n",
    "\n",
    "**[Swiss Federal Office of Energy](http://www.bfe.admin.ch/themen/00612/02073/index.html?dossier_id=02166&lang=de)** - Data of all renewable power plants receiving \"Kostendeckende Einspeisevergütung\" (KEV) which is the Swiss feed in tarif for renewable power plants. \n",
    "Geodata is based on municipality codes.\n",
    "\n",
    "The available municipality code in the original data provides an approximation for the geocoordinates of the renewable power plants. The postcode will be assigned to latitude and longitude coordinates with the help of the postcode table.\n",
    "\n",
    "**[geonames.org](http://download.geonames.org/export/zip/?C=N;O=D)** - The postcode  data from Switzerland is provided by Geonames and licensed under a [Creative Commons Attribution 3.0 license](http://creativecommons.org/licenses/by/3.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and get the local paths of the downloaded files\n",
    "filepaths = downloader.download_data_for_country('CH')\n",
    "CH_re_filepath = filepaths['BFE']\n",
    "CH_geo_filepath = filepaths['Geonames']   \n",
    "CH_postcode2nuts_filepath = filepaths['Eurostat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of renewables per municipality\n",
    "CH_re_df = pd.read_excel(CH_re_filepath,\n",
    "                         sheet_name='KEV Bezüger 2017',\n",
    "                         encoding='UTF8',\n",
    "                         thousands='.',\n",
    "                         decimals=','\n",
    "                         #header=[0]\n",
    "                         #skipfooter=9,  # contains summarized values\n",
    "                         #index_col=[0, 1], # required for MultiIndex\n",
    "                         #converters={'Code officiel géographique':str}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Switzerland, create dictionary and show dictionary\n",
    "idx_CH = columnnames[columnnames['country'] == 'CH'].index\n",
    "column_dict_CH = columnnames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()\n",
    "column_dict_CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate columnnames\n",
    "CH_re_df.columns = [column_name.replace(\"\\n\", \"\") for column_name in CH_re_df.columns]\n",
    "CH_re_df.rename(columns=column_dict_CH, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_re_df['data_source'] = 'BFE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonize energy source hierarchy and translate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Switzerland, create dictionary \n",
    "idx_CH = valuenames[valuenames['country'] == 'CH'].index\n",
    "value_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate and assign energy source level 1-3 and technology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign energy_source_level_1 to the dataframe\n",
    "CH_re_df['energy_source_level_1'] = 'Renewable energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary in order to assign energy_source to its subtype\n",
    "#energy_source_dict_CH = valuenames.loc[idx_CH].set_index('opsd_name')['energy_source_level_2'].to_dict()\n",
    "#\n",
    "# ...and the energy source subtype values in the energy_source column are replaced by \n",
    "# the higher level classification\n",
    "#CH_re_df['energy_source_level_2'].replace(energy_source_dict_CH, inplace=True)\n",
    "\n",
    "CH_re_df['energy_source_level_3'] = CH_re_df['technology']\n",
    "\n",
    "# Create dictionnary in order to assign energy_source_level_2 to its subtype\n",
    "idx_CH = valuenames[valuenames['country'] == 'CH'].index\n",
    "energy_source_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "# Add energy_source_level_2\n",
    "CH_re_df['energy_source_level_2'] = CH_re_df['energy_source_level_2'].replace(energy_source_dict_CH)\n",
    "\n",
    "# Translate values in order to standardize energy_source_level_3\n",
    "value_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()\n",
    "\n",
    "\n",
    "CH_re_df['energy_source_level_3'].replace(value_dict_CH, inplace=True)\n",
    "\n",
    "# Standardize the values for technology\n",
    "# 1. np.nan means that technology should not be specified for the respective kind of sources\n",
    "#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n",
    "# 2. 'Other or unspecified technology' means that technology should be specified\n",
    "#    but it was unclear or missing in the original dataset.\n",
    "technology_translation_dictionary = {\n",
    "    'Klärgasanlage': np.nan,\n",
    "    'übrige Biomasse - WKK-Anlage': 'Other or unspecified technology',\n",
    "    'übrige Biomasse - Dampfprozess': 'Steam turbine',\n",
    "    'Schlammverbrennungsanlage': 'Combustion engine',\n",
    "    'WKK-Prozess': 'Other or unspecified technology',\n",
    "    'Kehrrichtverbrennungsanlage': 'Combustion engine',\n",
    "    'Integrierte Anlage': 'Photovoltaics',\n",
    "    'Angebaute Anlage': 'Photovoltaics',\n",
    "    'Freistehende Anlage': 'Photovoltaics',\n",
    "    'Trinkwasserkraftwerk': 'Other or unspecified technology',\n",
    "    'Durchlaufkraftwerk': 'Run-of-river',\n",
    "    'Dotierwasserkraftwerk': 'Other or unspecified technology',\n",
    "    'Ausleitkraftwerk': 'Other or unspecified technology',\n",
    "    'Wind Offshore': 'Other or unspecified technology',\n",
    "    'Abwasserkraftwerk': 'Other or unspecified technology',\n",
    "    'Unbekannt': 'Other or unspecified technology',\n",
    "    np.nan: 'Onshore',\n",
    "    None: 'Onshore'\n",
    "}\n",
    "\n",
    "CH_re_df['technology'].replace(technology_translation_dictionary, inplace=True)\n",
    "\n",
    "# Add energy_source_level_1\n",
    "CH_re_df['energy_source_level_1'] = 'Renewable energy'\n",
    "\n",
    "# Show the hierarchy of sources present in the dataset\n",
    "CH_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates().sort_values(by='energy_source_level_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_re_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace the rest of the original terms with their OPSD equivalents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_re_df.replace(value_dict_CH, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing\n",
    "\n",
    "#### Postcode to lat/lon (WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geo-information\n",
    "zip_CH_geo = zipfile.ZipFile(CH_geo_filepath)\n",
    "\n",
    "# Read generated postcode/location file\n",
    "CH_geo = pd.read_csv(zip_CH_geo.open('CH.txt'), sep='\\t', header=-1)\n",
    "\n",
    "# add column names as defined in associated readme file\n",
    "CH_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n",
    "                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n",
    "                  'admin_code3', 'lat', 'lon', 'accuracy']\n",
    "\n",
    "# Drop rows of possible duplicate postal_code\n",
    "CH_geo.drop_duplicates('postcode', keep='last', inplace=True)\n",
    "CH_geo['postcode'] = CH_geo['postcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonise data class \n",
    "CH_geo.postcode = CH_geo.postcode.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add longitude/latitude infomation assigned by municipality code\n",
    "CH_re_df = pd.merge(CH_re_df,\n",
    "                    CH_geo[['lat', 'lon', 'postcode']],\n",
    "                    left_on='municipality_code',\n",
    "                    right_on='postcode',\n",
    "                    how='left'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_CH_geo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add NUTS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_postcode2nuts_path = filepaths['Eurostat']\n",
    "\n",
    "# Use the string versions of postcode and municipality code columns\n",
    "CH_re_df['postcode_str'] = CH_re_df['postcode'].astype(str).str[:-2]\n",
    "CH_re_df['municipality_code_str'] = CH_re_df['municipality_code'].astype(str)\n",
    "\n",
    "CH_re_df = nuts_converter.add_nuts_information(CH_re_df, 'CH', CH_postcode2nuts_path, \n",
    "                                                postcode_column='postcode_str',\n",
    "                                                municipality_code_column='municipality_code_str',\n",
    "                                                lau_name_type='NATIONAL', how=['postcode', 'municipality'])\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = CH_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', CH_re_df.shape[0], 'facilities in CH.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = CH_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', CH_re_df.shape[0], 'facilities in CH.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the stations for which NUTS could not be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_re_df[CH_re_df['nuts_1_region'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform electrical_capacity from kW to MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kW to MW\n",
    "CH_re_df['electrical_capacity'] /= 1000\n",
    "\n",
    "# kWh to MWh\n",
    "CH_re_df['production'] /= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['project_name', 'energy_source_level_2','energy_source_level_3', 'technology', \n",
    "                   'electrical_capacity', 'production', 'tariff', 'commissioning_date', 'contract_period_end',\n",
    "                   'street', 'municipality_code', 'municipality', 'nuts_1_region', 'nuts_2_region',\n",
    "                   'nuts_3_region', 'canton', 'company', 'title', 'surname', 'first_name', 'data_source',\n",
    "                   'energy_source_level_1', 'lat', 'lon', 'postcode']\n",
    "CH_re_df = CH_re_df.loc[:, columns_to_keep]\n",
    "CH_re_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(CH_re_df['lat'],\n",
    "                 CH_re_df['lon'],\n",
    "                 'Switzerland',\n",
    "                 categories=CH_re_df['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_re_df.to_pickle('intermediate/CH_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and validation of the renewable power plants list as well as the creation of CSV/XLSX/SQLite files can be found in Part 2 of this script. It also generates a daily time series of cumulated installed capacities by energy source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United Kingdom UK\n",
    "\n",
    "The data for the UK are provided by the following sources:\n",
    "\n",
    "**[UK Government Department of Business, Energy & Industrial Strategy (BEIS)](https://www.gov.uk/government/publications/renewable-energy-planning-database-monthly-extract)** - the data contain information on the UK renewable energy sources and are updated at the end of each quarter.\n",
    "\n",
    "**[geonames.org](http://download.geonames.org/export/zip/?C=N;O=D)** - the data about latitued and longitudes of  the UK postcodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and get the local paths to the corresponding files\n",
    "filepaths = downloader.download_data_for_country('UK')\n",
    "UK_re_filepath = filepaths['BEIS']\n",
    "UK_geo_filepath = filepaths['Geonames']\n",
    "UK_postcode2nuts_filepath = filepaths['Eurostat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the renewable powerplants data into a dataframe\n",
    "UK_re_df = pd.read_csv(UK_re_filepath,\n",
    "                       header=6,\n",
    "                       encoding='latin1',\n",
    "                       parse_dates=['Record Last Updated (dd/mm/yyyy)', 'Operational'],\n",
    "                       infer_datetime_format=True,\n",
    "                       thousands=',',\n",
    "                       )\n",
    "\n",
    "# Drop empty columns and rows\n",
    "UK_re_df.dropna(axis='index', how='all', inplace=True)\n",
    "UK_re_df.dropna(axis='columns', how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "The downloaded dataset has to be cleaned:\n",
    "  - Both operational and nonoperational facilities are present in the set. However, only operational facilities are of the interest, so the dataset has to be filtered on this condition. \n",
    "  - Some columns don't have standardized values. For example, `CHP Enabled` contains five different strings: \"`No`\", \"`Yes`\", \"`no`\", \"`yes`\", and \"`No` \" with a trailing white space, even though they represent only two distinct values. So, we have to ensure a 1-to-1 mapping between the true values of a feature and their representations for all the features present in the set.\n",
    "  - The technologies `Battery` and `Flywheels` are of no interest, so the facilities using them should be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only operational facilities in the dataset\n",
    "UK_re_df = UK_re_df.loc[UK_re_df[\"Development Status\"] == \"Operational\"]\n",
    "UK_re_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize string columns\n",
    "strip_and_lower = ['CHP Enabled']\n",
    "strip_only = ['Country', 'County', 'Operator (or Applicant)', 'Mounting Type for Solar']\n",
    "\n",
    "for column in strip_and_lower:\n",
    "    util.helper.standardize_column(UK_re_df, column, lower=True)\n",
    "\n",
    "for column in strip_only:\n",
    "    util.helper.standardize_column(UK_re_df, column, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Flywheels and Battery\n",
    "UK_re_df = UK_re_df[~UK_re_df['Technology Type'].isin(['Flywheels', 'Battery'])]\n",
    "UK_re_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the column \"Technology Type\" to a new column named \"technology\"\n",
    "UK_re_df['technology'] = UK_re_df['Technology Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for the UK and create the translation dictionary\n",
    "idx_UK = columnnames[columnnames['country'] == 'UK'].index\n",
    "column_dict_UK = columnnames.loc[idx_UK].set_index('original_name')['opsd_name'].to_dict()\n",
    "\n",
    "# Show the dictionary\n",
    "column_dict_UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate column names\n",
    "UK_re_df.rename(columns=column_dict_UK, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_re_df['data_source'] = 'BEIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonise energy source levels 1-3 and technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary in order to assign energy_source_level_2 to its subtype\n",
    "idx_UK = valuenames[valuenames['country'] == 'UK'].index\n",
    "energy_source_dict_UK = valuenames.loc[idx_UK].set_index('original_name')['energy_source_level_2'].to_dict()\n",
    "\n",
    "# Add energy_source_level_2\n",
    "UK_re_df['energy_source_level_2'] = UK_re_df['energy_source_level_3'].replace(energy_source_dict_UK)\n",
    "\n",
    "# Translate values in order to standardize energy_source_level_3\n",
    "value_dict_UK = valuenames.loc[idx_UK].set_index('original_name')['opsd_name'].to_dict()\n",
    "UK_re_df['energy_source_level_3'].replace(value_dict_UK, inplace=True)\n",
    "\n",
    "# Standardize the values for technology\n",
    "# 1. np.nan means that technology should not be specified for the respective kind of sources\n",
    "#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n",
    "# 2. 'Other or unspecified technology' means that technology should be specified\n",
    "#    but it was unclear or missing in the original dataset.\n",
    "technology_translation_dictionary = {\n",
    "    'Biomass (co-firing)': 'Other or unspecified technology',\n",
    "    'Biomass (dedicated)': 'Other or unspecified technology',\n",
    "    'Advanced Conversion Technologies': 'Other or unspecified technology',\n",
    "    'Anaerobic Digestion': 'Other or unspecified technology',\n",
    "    'EfW Incineration': np.nan,\n",
    "    'Large Hydro': 'Other or unspecified technology',\n",
    "    'Small Hydro': 'Other or unspecified technology',\n",
    "    'Landfill Gas': np.nan,\n",
    "    'Solar Photovoltaics': 'Photovoltaics',\n",
    "    'Sewage Sludge Digestion': np.nan,\n",
    "    'Tidal Barrage and Tidal Stream': np.nan,\n",
    "    'Shoreline Wave': np.nan,\n",
    "    'Wind Offshore': 'Offshore',\n",
    "    'Wind Onshore': 'Onshore',\n",
    "    'Pumped Storage Hydroelectricity': 'Pumped storage'\n",
    "}\n",
    "UK_re_df['technology'].replace(technology_translation_dictionary, inplace=True)\n",
    "\n",
    "# Add energy_source_level_1\n",
    "UK_re_df['energy_source_level_1'] = 'Renewable energy'\n",
    "\n",
    "# Show the hierarchy of sources present in the dataset\n",
    "UK_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Georeferencing\n",
    "\n",
    "The facilities' location details comprise of the information on the address, county, region, country (England, Scotland, Wales, Northern Ireland), post code, and Easting (X) and Northing (Y) coordinates of each facility in the OSGB georeferencing system. To convert the easting and northing cordinates to standard WG84 latitude and longitude, we use package `bng_latlon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper for bng_to_latlon for handling None values \n",
    "def to_lat_lon(easting, northing):\n",
    "    if pd.isnull(easting) or pd.isnull(northing):\n",
    "        return (None, None)\n",
    "    else:\n",
    "        return bng_to_latlon.OSGB36toWGS84(easting, northing)\n",
    "\n",
    "# Convert easting and northing columns to numbers\n",
    "UK_re_df['X-coordinate'] = pd.to_numeric(\n",
    "                             UK_re_df['X-coordinate'].astype(str).str.replace(',', ''),\n",
    "                             errors='coerce'\n",
    "                           )\n",
    "UK_re_df['Y-coordinate'] = pd.to_numeric(\n",
    "                             UK_re_df['Y-coordinate'].astype(str).str.replace(',', ''),\n",
    "                             errors='coerce'\n",
    "                           )\n",
    "\n",
    "# Convert easting and northing coordinates to standard latitude and longitude\n",
    "latlon = UK_re_df.apply(lambda row: to_lat_lon(row[\"X-coordinate\"], row[\"Y-coordinate\"]),\n",
    "                        axis=1\n",
    "                       )\n",
    "\n",
    "# Split a column of (latitude, longitude) pairs into two separate coordinate columns\n",
    "latitude = latlon.apply(lambda x: x[0])\n",
    "longitude = latlon.apply(lambda x: x[1])\n",
    "\n",
    "# Add them to the dataframe\n",
    "UK_re_df['latitude'] = latitude\n",
    "UK_re_df['longitude'] = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases with unknown Easting and Northing coordinates\n",
    "\n",
    "If the Easting and Northing coordinates of a facility are not provided, its latitude and longitude cannot be determined. For such sources, we look up the WGS84 coordinates in the geodataset provided by **[geonames.org](http://download.geonames.org/export/zip/?C=N;O=D)**, where the UK postcodes are paired with their latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geo-information\n",
    "zip_UK_geo = zipfile.ZipFile(UK_geo_filepath)\n",
    "\n",
    "# Read generated postcode/location file\n",
    "UK_geo = pd.read_csv(zip_UK_geo.open('GB_full.txt'), sep='\\t', header=-1)\n",
    "\n",
    "# add column names as defined in associated readme file\n",
    "UK_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n",
    "                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n",
    "                  'admin_code3', 'lat', 'lon', 'accuracy']\n",
    "\n",
    "# Drop rows of possible duplicate postal_code\n",
    "UK_geo.drop_duplicates('postcode', keep='last', inplace=True)\n",
    "UK_geo['postcode'] = UK_geo['postcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows where latitude and longitude are unknown\n",
    "missing_latlon_mask = UK_re_df['latitude'].isna() | UK_re_df['longitude'].isna()\n",
    "missing_latlon = UK_re_df[missing_latlon_mask]\n",
    "\n",
    "# Add longitude/latitude infomation assigned by post code\n",
    "updated_latlon = pd.merge(missing_latlon,\n",
    "                  UK_geo[['lat', 'lon', 'postcode']],\n",
    "                  left_on='postcode',\n",
    "                  right_on='postcode',\n",
    "                  how='left'\n",
    "                 )\n",
    "\n",
    "# Return the updated rows to the original frame\n",
    "UK_re_df = pd.merge(UK_re_df,\n",
    "             updated_latlon[['uk_beis_id', 'lat', 'lon']],\n",
    "             on='uk_beis_id',\n",
    "             how='left'\n",
    ")\n",
    "\n",
    "# Use the bng_to_latlon coordinates (columns: 'latitude' and 'longitude') if present, \n",
    "# otherwise, use those obtained with UK_geo (columns: 'lat' and 'lon').\n",
    "UK_re_df['longitude'] = UK_re_df.apply(lambda row: row['longitude'] if not pd.isnull(row['longitude']) \n",
    "                                                                    else row['lon'],\n",
    "                         axis=1\n",
    "                        )\n",
    "UK_re_df['latitude'] = UK_re_df.apply(lambda row: row['latitude'] if not pd.isnull(row['latitude']) \n",
    "                                                                  else row['lat'],\n",
    "                         axis=1\n",
    "                        )\n",
    "\n",
    "# Drop the UK_geo columns (lat/lon)\n",
    "# as the information was moved to the 'latitude' and 'longitude' columns.\n",
    "UK_re_df.drop(['lat', 'lon'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_UK_geo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases for approximation\n",
    "\n",
    "In the cases where the full post code was not present in **[geonames.org](http://download.geonames.org/export/zip/?C=N;O=D)**, use its prefix to find the latitude / longitude pairs of locations covered by that prefix. Then, approximate those facilities' locations by the centroids of their prefix areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows where latitude and longitude are unknown\n",
    "missing_latlon_mask = UK_re_df['latitude'].isna() | UK_re_df['longitude'].isna()\n",
    "missing_latlon = UK_re_df[missing_latlon_mask].copy()\n",
    "missing_latlon = missing_latlon.reset_index()\n",
    "\n",
    "# Determine their post code prefixes\n",
    "prefixes = missing_latlon.apply(lambda row: str(row['postcode']).split(' ')[0],\n",
    "                                axis=1\n",
    "                               )\n",
    "missing_latlon['Prefix'] = prefixes\n",
    "\n",
    "# Determine the centroids of the areas covered by the prefixes\n",
    "grouped_UK_geo=UK_geo.groupby(by=lambda i: str(UK_geo['postcode'].loc[i]).split(' ')[0])\n",
    "\n",
    "# Assing the centroid coordinates to the facilities with unknown coordinates\n",
    "updated_latlon = pd.merge(missing_latlon,\n",
    "                    grouped_UK_geo.mean(),\n",
    "                    left_on=\"Prefix\",\n",
    "                    right_index=True,\n",
    "                    how=\"left\"\n",
    "                   )\n",
    "\n",
    "# Return the updated rows to the original frame\n",
    "UK_re_df = pd.merge(UK_re_df,\n",
    "             updated_latlon[['uk_beis_id', 'lat', 'lon']],\n",
    "             on='uk_beis_id',\n",
    "             how='left'\n",
    ")\n",
    "\n",
    "# Keep the already known coordinates (columns: 'latitude' and 'longitude') if present, \n",
    "# otherwise, use those obtained by approximation (columns: 'lat' and 'lon').\n",
    "UK_re_df['longitude'] = UK_re_df.apply(lambda row: row['longitude'] if not pd.isnull(row['longitude']) \n",
    "                                                                    else row['lon'],\n",
    "                         axis=1\n",
    "                        )\n",
    "UK_re_df['latitude'] = UK_re_df.apply(lambda row: row['latitude'] if not pd.isnull(row['latitude']) \n",
    "                                                                  else row['lat'],\n",
    "                         axis=1\n",
    "                        )\n",
    "\n",
    "# Drop the UK_geo columns (lat/lon)\n",
    "# as the information was moved to the 'latitude' and 'longitude' columns.\n",
    "UK_re_df.drop(['lat', 'lon'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add NUTS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_postcode2nuts_filepath = filepaths['Eurostat']\n",
    "\n",
    "UK_re_df = nuts_converter.add_nuts_information(UK_re_df, 'UK', UK_postcode2nuts_filepath, \n",
    "                                                latitude_column='latitude',\n",
    "                                                longitude_column='longitude', closest_approximation=True,\n",
    "                                                lau_name_type='NATIONAL', how=['latlon', 'municipality'])\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = UK_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', UK_re_df.shape[0], 'facilities in UK.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = UK_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', UK_re_df.shape[0], 'facilities in UK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the facilities for which the NUTS codes could not be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_re_df[UK_re_df['nuts_1_region'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(UK_re_df['latitude'],\n",
    "                 UK_re_df['longitude'],\n",
    "                 'United Kingdom',\n",
    "                 categories=UK_re_df['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some facilities appear to be located in the sea. Let us plot the original OSGB coordinates to see if translation to the standard longitude and latitude coordinates failed for some locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_X = UK_re_df['X-coordinate'].max()\n",
    "min_X = UK_re_df['X-coordinate'].min()\n",
    "\n",
    "max_Y = UK_re_df['Y-coordinate'].max()\n",
    "min_Y = UK_re_df['Y-coordinate'].min()\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "ax = plt.axes(projection=ccrs.OSGB())\n",
    "ax.coastlines('10m')\n",
    "\n",
    "ax.scatter(UK_re_df['X-coordinate'], UK_re_df['Y-coordinate'],s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the maps are basically the same, which confirms that translation to the longitude and latitude coordinates is done correctly and that they reflect the positions specified by the original X and Y OSGB coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'longitude' and 'latitude' to 'lon' and 'lat' to conform to the naming convention\n",
    "# used for other countries.\n",
    "UK_re_df.rename(columns={'longitude': 'lon', 'latitude': 'lat'}, inplace=True)\n",
    "\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_of_interest = ['commissioning_date', 'uk_beis_id', 'operator', 'site_name',\n",
    "                       'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology',\n",
    "                       'electrical_capacity', 'chp', 'support_robranding', 'support_fit', 'support_cfd',\n",
    "                       'capacity_individual_turbine', 'number_of_turbines', 'solar_mounting_type',\n",
    "                       'status', 'address', 'municipality', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n",
    "                       'region', 'country', 'postcode', 'lon', 'lat', 'data_source'\n",
    "                      ]\n",
    "\n",
    "for col in columns_of_interest:\n",
    "    if col not in UK_re_df.columns:\n",
    "        print(col)\n",
    "\n",
    "# Clean the dataframe from columns other than those specified above\n",
    "UK_re_df = UK_re_df.loc[:, columns_of_interest]\n",
    "UK_re_df.reset_index(drop=True, inplace=True)\n",
    "UK_re_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_re_df.to_pickle('intermediate/UK_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweden\n",
    "\n",
    "The data for Sweden are provided by the following sources:\n",
    "\n",
    "- **[Vindbrukskollen](http://ext-dokument.lansstyrelsen.se/Gemensamt/Geodata/Externa%20dokument/VBK/VBK_export_allman_prod.xlsx)** - Wind farms in Sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and get the local paths to the corresponding files\n",
    "filepaths = downloader.download_data_for_country('SE')\n",
    "print(filepaths)\n",
    "\n",
    "SE_re_filepath = filepaths['Vindbrukskollen']\n",
    "SE_geo_filepath = filepaths['Geonames']\n",
    "SE_postcode2nuts_filepath = filepaths['Eurostat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for converting the column \"Senast sparads\" to date type\n",
    "def from_int_to_date(int_date):\n",
    "    str_date =str (int_date)\n",
    "    year = str_date[:4]\n",
    "    month = str_date[4:6]\n",
    "    day = str_date[6:8]\n",
    "    str_date = '{}/{}/{}'.format(year, month, day)\n",
    "    return pd.to_datetime(str_date, format='%Y/%m/%d')\n",
    "    \n",
    "# Read the data\n",
    "SE_re_df = pd.read_excel(SE_re_filepath,\n",
    "                        sheet_name='Vindkraftverk',\n",
    "                        na_values='-',\n",
    "                        parse_dates=['Uppfört', 'Senast sparad'],\n",
    "                        infer_datetime_format=True,\n",
    "                        converters={'Senast sparad' : from_int_to_date}\n",
    "                        )\n",
    "\n",
    "# Show 5 rows from the beginning\n",
    "SE_re_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    " - Drop empty rows and columns.\n",
    " - Make sure that the column `Uppfört` is of the date type.\n",
    " - Keep only operational wind farms (`Status` is `Beviljat` (permission granted) or `Uppfört` (the farm exists)).\n",
    " - Remove the farms whose capacity is not known.\n",
    " - Standardize string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty rows and columns \n",
    "SE_re_df.dropna(axis='index', how='all', inplace=True)\n",
    "SE_re_df.dropna(axis='columns', how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the column Uppfört is of the date type\n",
    "SE_re_df['Uppfört'] = pd.to_datetime(SE_re_df['Uppfört'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only operational wind farms\n",
    "subset_mask = SE_re_df['Status'].isin(['Beviljat', 'Uppfört'])\n",
    "SE_re_df.drop(SE_re_df[~subset_mask].index, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the farms whose capacity is not known.\n",
    "subset_mask = SE_re_df['Maxeffekt (MW)'].isna()\n",
    "SE_re_df.drop(SE_re_df[subset_mask].index, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize string columns\n",
    "string_columns = ['Modell', 'Fabrikat', 'Elområde', 'Kommun', 'Län', 'Handlingstyp', 'Placering']\n",
    "for col in string_columns:\n",
    "    util.helper.standardize_column(SE_re_df, col, lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for the UK and create the translation dictionary\n",
    "idx_SE = columnnames[columnnames['country'] == 'SE'].index\n",
    "column_dict_SE = columnnames.loc[idx_SE].set_index('original_name')['opsd_name'].to_dict()\n",
    "\n",
    "# Show the dictionary\n",
    "display(column_dict_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate column names\n",
    "SE_re_df.rename(columns=column_dict_SE, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_re_df['data_source'] = 'Vindbrukskollen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy source levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Sweden\n",
    "idx_SE = valuenames[valuenames['country'] == 'SE'].index\n",
    "value_dict_SE = valuenames.loc[idx_SE].set_index('original_name')['opsd_name'].to_dict()\n",
    "value_dict_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all original value names by the OPSD value names\n",
    "SE_re_df.replace(value_dict_SE, inplace=True)\n",
    "\n",
    "# Set nans in the technology column to 'Unknown or unspecified technology'\n",
    "SE_re_df['technology'].fillna('Unknown or unspecified technology', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add energy level 2\n",
    "SE_re_df['energy_source_level_2'] = 'Wind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add energy_source_level_1\n",
    "SE_re_df['energy_source_level_1'] = 'Renewable energy'\n",
    "\n",
    "# Show the hierarchy of sources present in the dataset\n",
    "SE_re_df[['energy_source_level_1', 'energy_source_level_2', 'technology']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing\n",
    "\n",
    "The coordinates in the columns `sweref99tm_north` and `sweref99tm_east` are specified in the [SWEREF 99 TM](https://sv.wikipedia.org/wiki/SWEREF_99_TM) coordinate system, used in Sweden. To convert those coordinates to the usual [WGS84](https://en.wikipedia.org/wiki/World_Geodetic_System) latitudes and longitudes, we use the function `sweref99tm_latlon_transform` from the module `util.helper`, provided by Jon Olauson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latitude and longitude columns\n",
    "lat, lon = util.helper.sweref99tm_latlon_transform(SE_re_df['sweref99tm_north'], SE_re_df['sweref99tm_east'])\n",
    "\n",
    "# Include them in the dataframe\n",
    "SE_re_df['lat'] = lat\n",
    "SE_re_df['lon'] = lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning NUTS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_postcode2nuts_filepath = filepaths['Eurostat']\n",
    "\n",
    "SE_re_df = nuts_converter.add_nuts_information(SE_re_df, 'SE', SE_postcode2nuts_filepath, \n",
    "                                                lau_name_type='NATIONAL', how=['municipality', 'latlon'])\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = SE_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', SE_re_df.shape[0], 'facilities in SE.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = SE_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', SE_re_df.shape[0], 'facilities in SE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns should be kept\n",
    "columns_to_keep = ['municipality', 'county', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region', 'lat', 'lon',\n",
    "           'energy_source_level_1', 'energy_source_level_2', 'technology', 'se_vindbrukskollen_id',\n",
    "            'site_name', 'manufacturer',\n",
    "           'electrical_capacity', 'commissioning_date', 'data_source']\n",
    "# Keep only the selected columns\n",
    "SE_re_df = SE_re_df.loc[:, columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(SE_re_df['lat'],\n",
    "                 SE_re_df['lon'],\n",
    "                 'Sweden',\n",
    "                 categories=SE_re_df['technology']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_re_df.reset_index(inplace=True, drop=True)\n",
    "SE_re_df.to_pickle('intermediate/SE_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czech Republic\n",
    "\n",
    "The data for Czech Republic are provided by the following source:\n",
    "-  **[ERU (Energetický regulační úřad, Energy Regulatory Office)](http://www.eru.cz/en/)** - Administrative authority responsible for regulation in the energy sector. Provides the data on renewable energy plants in Czech Republic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read the data\n",
    "\n",
    "Downloading the data from the original source may take 1-2 hours because it's done by scraping the information from HTML pages. \n",
    "\n",
    "If downloading fails because of the ERU's server refusing connections:\n",
    "- pause and wait for some time;\n",
    "- delete the file `eru.csv` in the CZ input directory;\n",
    "- try downloading again.\n",
    "\n",
    "Alternatively, you can download the data from the OPSD server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and get the local paths to the corresponding files\n",
    "print('Start:', datetime.datetime.now())\n",
    "downloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n",
    "filepaths = downloader.download_data_for_country('CZ')\n",
    "print('End:', datetime.datetime.now())\n",
    "\n",
    "CZ_re_filepath = filepaths['ERU']\n",
    "CZ_geo_filepath = filepaths['Geonames']\n",
    "CZ_postcode2nuts_filepath = filepaths['Eurostat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a converter for CZ postcode strings\n",
    "def to_cz_postcode_format(postcode_str):\n",
    "    return postcode_str[:3] + ' ' + postcode_str[3:]\n",
    "\n",
    "# Read the data from the csv file\n",
    "CZ_re_df = pd.read_csv(CZ_re_filepath,\n",
    "                       escapechar='\\\\',\n",
    "                       dtype = {\n",
    "                           'number_of_sources' : int,\n",
    "                       },\n",
    "                       parse_dates=['licence_approval_date'],\n",
    "                       infer_datetime_format=True,\n",
    "                       converters = {\n",
    "                           'site_postcode' : to_cz_postcode_format,\n",
    "                           'holder_postcode' : to_cz_postcode_format\n",
    "                       }\n",
    "                      )\n",
    "# Show a few rows\n",
    "CZ_re_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dataframe's columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CZ_re_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 30 columns:\n",
    "- `site_name`, `site_region`, `site_postcode`, `site_locality`, `site_district` give us basic information on the site;\n",
    "- `megawatts_electric_total` shows us the total electric capacity of the site;\n",
    "- Since each site can use different types of energy, `megawatts_electric_hydro`, `megawatts_electric_solar`, `megawatts_electric_biogas_and_biomass`, `megawatts_electric_wind`, `megawatts_electric_unspecified` show us how total capacity breaks down to those renewable types from the OPSD energy hierarchy;\n",
    "- The columns beginning with `megawatts_thermal_` represent the amiunt of input energy required (and will be equal to zero in most cases);\n",
    "- `watercourse` and `watercourse_length_km` represent the name and length of the watercourse used by the site (if any);\n",
    "- `holder_name`, `holder_region`, `holder_address`, `holder_postcode`, `holder_locality`, `holder_district`, `holder_representative` give us basic information on the site's owner;\n",
    "- `licence_number` and `licence_approval_date` show us the licence number given to the holder and its approval date.\n",
    "- `link` points to the ERU page with the site's data in HTML.\n",
    "\n",
    "Since some sites use conventional types of energy, it is possible that `megawatts_electric_total > megawatts_electric_hydro + megawatts_electric_solar + megawatts_electric_biogas_and_biomass + megawatts_electric_wind + megawatts_electric_unspecified`. If the sum of renewable-energy capacities is equal to zero, that means that the correspoding row actually represents a conventional powerplant, so it should be excluded.\n",
    "\n",
    "Let us now check how many sites use how many types of renewable energy sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_columns = [col for col in CZ_re_df.columns if 'megawatts_electric' in col and col != 'megawatts_electric_total']\n",
    "mwt_columns = [col for col in CZ_re_df.columns if 'megawatts_thermal' in col and col != 'megawatts_thermal_total']\n",
    "\n",
    "def count_types(row):\n",
    "    global mwe_columns\n",
    "    different_types = sum([row[col] > 0 for col in mwe_columns])\n",
    "    return different_types\n",
    "\n",
    "CZ_re_df.apply(count_types, axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of September 2019, as we can see in the output above, there are only 4 sites which use more than one type of renewable energy, and there are 201 sites which do not use renewable energy at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty columns and rows\n",
    "CZ_re_df.dropna(axis='index', how='all', inplace=True)\n",
    "CZ_re_df.dropna(axis='columns', how='all', inplace=True)\n",
    "\n",
    "# Drop rows with no data on electrical capacity and the rows where total electrical capacity is 0\n",
    "empty_mask = (CZ_re_df['megawatts_electric_total'] == 0) | (CZ_re_df['megawatts_electric_total'].isnull())\n",
    "CZ_re_df = CZ_re_df.loc[~empty_mask]\n",
    "CZ_re_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Replace NANs with zeroes in mwe and mwt columns\n",
    "replacement_dict = {col : 0 for col in mwe_columns + mwt_columns}\n",
    "CZ_re_df.fillna(replacement_dict, inplace=True)\n",
    "\n",
    "# Drop the rows where renewable-energy share of the total capacity is equal to zero\n",
    "conventional_mask = (CZ_re_df['megawatts_electric_hydro'] +\n",
    "                     CZ_re_df['megawatts_electric_solar'] +\n",
    "                     CZ_re_df['megawatts_electric_biogas_and_biomass'] + \n",
    "                     CZ_re_df['megawatts_electric_wind'] + \n",
    "                     CZ_re_df['megawatts_electric_unspecified']) == 0\n",
    "\n",
    "CZ_re_df = CZ_re_df.loc[~conventional_mask]\n",
    "CZ_re_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the data\n",
    "\n",
    "There are sites which use different types of renewable source to produce electric energy. Those are the sites where at least two of the following columns are not equal to zero: `megawatts_electric_hydro`, `megawatts_electric_solar`, `megawatts_electric_biogas_and_biomass`, `megawatts_electric_wind`, `megawatts_electric_unspecified`. The data that come in this shape are said to be in the so called *wide format*. For the purpose of our later processing, it would be more convenient to have the data where each row is associated to one and only one type of energy (the so called *long format*). Therefore, we must first restructure our data from the wide to long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which will extract the data about the type of energy specified by the given column\n",
    "# and return it as a dataframe in the \"long format\"\n",
    "def select_and_reformat(df, column):\n",
    "    # Use the mwe and mwt columns defined above\n",
    "    global mwe_columns\n",
    "    global mwt_columns\n",
    "    \n",
    "    # Declare the given column and its mwt counterpart as exceptions\n",
    "    mwt_exception = column.replace('electric', 'thermal')\n",
    "    exceptions = [column, mwt_exception]\n",
    "\n",
    "    # Exclude all the mwe and mwt columns which do not correspond to the given energy type\n",
    "    columns_to_skip = [col for col in mwe_columns + mwt_columns if col not in exceptions]\n",
    "    # Keep all the other columns\n",
    "    columns_to_keep = [col for col in df.columns if col not in columns_to_skip]\n",
    "    \n",
    "    # Find the stations which use the given type of energy\n",
    "    selection_mask = (df[column] > 0)\n",
    "    \n",
    "    # Keep them and select the columns we decided to keep\n",
    "    selection_df = df[selection_mask][columns_to_keep]\n",
    "    \n",
    "    # Create a new column which will indicate the energy type\n",
    "    selection_df['energy_type'] = \" \".join(column.split('_')[2:])\n",
    "    \n",
    "    # Remove the energy type name from the columns representing electrical capacity\n",
    "    # and megawatts thermal\n",
    "    selection_df.rename(columns = {column : 'electrical_capacity',\n",
    "                                   mwt_exception : 'megawatts_thermal'},\n",
    "                        inplace=True)\n",
    "    selection_df.drop(columns=['megawatts_electric_total', 'megawatts_thermal_total'],\n",
    "                     inplace=True)\n",
    "    \n",
    "    # Ensure the rows are properly indexed as 0,1,2,...\n",
    "    selection_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return selection_df\n",
    "\n",
    "# Create a dataframe for each energy type\n",
    "dataframes = []\n",
    "for column in mwe_columns:\n",
    "    selection = select_and_reformat(CZ_re_df, column)\n",
    "    energy_type = selection['energy_type'].unique()[0]\n",
    "    dataframes.append(selection)\n",
    "\n",
    "# Concatenate the dataframes\n",
    "CZ_re_df = pd.concat(dataframes, ignore_index=False)\n",
    "CZ_re_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what is this restructured dataframe like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CZ_re_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns has been reduced as we have transformed the data to the long format. The rows representning conventional power plants have been excluded. Since only few sites use multiple types of energy, the total number of rows has not increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for CZ and create the translation dictionary\n",
    "idx_CZ = columnnames[columnnames['country'] == 'CZ'].index\n",
    "column_dict_CZ = columnnames.loc[idx_CZ].set_index('original_name')['opsd_name'].to_dict()\n",
    "\n",
    "# Show the dictionary\n",
    "column_dict_CZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate column names\n",
    "CZ_re_df.rename(columns=column_dict_CZ, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate values and harmonize energy levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for Czech Republic\n",
    "idx_CZ = valuenames[valuenames['country'] == 'CZ'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the translation terms for energy source level 3\n",
    "energy3_dict_CZ = valuenames.loc[idx_CZ].set_index('original_name')['opsd_name'].to_dict()\n",
    "energy3_dict_CZ\n",
    "\n",
    "# Add energy source level 3\n",
    "CZ_re_df['energy_source_level_3'] = CZ_re_df['technology'].replace(energy3_dict_CZ)\n",
    "\n",
    "# Choose the terms for energy source level 2\n",
    "energy2_dict_CZ = valuenames.loc[idx_CZ].set_index('original_name')['energy_source_level_2'].to_dict()\n",
    "CZ_re_df['energy_source_level_2'] = CZ_re_df['technology'].replace(energy2_dict_CZ)\n",
    "\n",
    "# Standardize the values for technology\n",
    "# 1. np.nan means that technology should not be specified for the respective kind of sources\n",
    "#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n",
    "# 2. 'Other or unspecified technology' means that technology should be specified\n",
    "#    but it was unclear or missing in the original dataset.\n",
    "technology_dict = {\n",
    "    'biogas and biomass' : np.nan,\n",
    "    'wind' : 'Onshore',\n",
    "    'solar' : 'Other or unspecified technology',\n",
    "    'hydro' : 'Run-of-river',\n",
    "    'unspecified' : np.nan\n",
    "}\n",
    "CZ_re_df['technology'] = CZ_re_df['technology'].replace(technology_dict)\n",
    "\n",
    "# Add energy_source_level_1\n",
    "CZ_re_df['energy_source_level_1'] = 'Renewable energy'\n",
    "\n",
    "# Show the hierarchy of sources present in the dataset\n",
    "CZ_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CZ_re_df['data_source'] = 'ERU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georeferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geo-information\n",
    "zip_CZ_geo = zipfile.ZipFile(CZ_geo_filepath)\n",
    "\n",
    "# Read generated postcode/location file\n",
    "CZ_geo = pd.read_csv(zip_CZ_geo.open('CZ.txt'), sep='\\t', header=-1)\n",
    "\n",
    "# add column names as defined in associated readme file\n",
    "CZ_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n",
    "                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n",
    "                  'admin_code3', 'lat', 'lon', 'accuracy']\n",
    "\n",
    "# Drop rows of possible duplicate postal_code\n",
    "CZ_geo.drop_duplicates('postcode', keep='last', inplace=True)\n",
    "\n",
    "# Add longitude/latitude infomation assigned by postcode\n",
    "CZ_re_df = pd.merge(CZ_re_df,\n",
    "                    CZ_geo[['lat', 'lon', 'postcode']],\n",
    "                    left_on='postcode',\n",
    "                    right_on='postcode',\n",
    "                    how='left'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign NUTS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CZ_postcode2nuts_filepath = filepaths['Eurostat']\n",
    "\n",
    "CZ_re_df = nuts_converter.add_nuts_information(CZ_re_df, 'CZ', CZ_postcode2nuts_filepath, how=['postcode'])\n",
    "\n",
    "# Report the number of facilites whose NUTS codes were successfully sudetermined\n",
    "determined = CZ_re_df['nuts_1_region'].notnull().sum()\n",
    "print('NUTS successfully determined for', determined, 'out of', CZ_re_df.shape[0], 'facilities in CZ.')\n",
    "\n",
    "# Report the number of facilites whose NUTS codes could not be determined\n",
    "not_determined = CZ_re_df['nuts_1_region'].isnull().sum()\n",
    "print('NUTS could not be determined for', not_determined, 'out of', CZ_re_df.shape[0], 'facilities in CZ.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns should be kept\n",
    "columns_to_keep = ['site_name', 'region', 'municipality', 'locality', 'postcode',\n",
    "                   'nuts_1_region', 'nuts_2_region', 'nuts_3_region', 'lat', 'lon',\n",
    "                   'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology', \n",
    "                   'owner', 'electrical_capacity',  'data_source']\n",
    "\n",
    "# Keep only the selected columns\n",
    "CZ_re_df = CZ_re_df.loc[:, columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualuze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(CZ_re_df['lat'],\n",
    "                 CZ_re_df['lon'],\n",
    "                 'Czechia',\n",
    "                 categories=CZ_re_df['energy_source_level_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CZ_re_df.reset_index(inplace=True, drop=True)\n",
    "CZ_re_df.to_pickle('intermediate/CZ_renewables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_archive = zipfile.ZipFile(input_directory_path + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "print(\"Zipping the raw files...\")\n",
    "for filename in os.listdir(input_directory_path):\n",
    "    print(\"Adding\", filename, \"to the zip.\")\n",
    "    filepath = os.path.join(input_directory_path, filename)\n",
    "    zip_archive.write(filepath)\n",
    "zip_archive.close()\n",
    "print(\"Done!\")\n",
    "#shutil.rmtree(input_directory_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "p3",
   "language": "python",
   "name": "p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "716px",
    "left": "104px",
    "top": "280px",
    "width": "231px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
